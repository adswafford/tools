{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# conda command to install dependencies for amplicon and stool files (no host depletion needed):\n",
    "#   conda create -n ebi_sra_importer pandas requests entrez-direct sra-tools\n",
    "#   xmltodict lxml pyyaml xlrd -c bioconda -c conda-forge -y\n",
    "# to enable host depletion use:\n",
    "#   conda create -n ebi_sra_importer pandas requests entrez-direct sra-tools\n",
    "#   bowtie2 minimap2 samtools bedtools xmltodict lxml pyyaml xlrd fastqc multiqc\n",
    "#   -c bioconda-c conda-forge -y\n",
    "# or to enable later after installation:\n",
    "#   conda activate ebi_sra_importer\n",
    "#   conda install bowtie2 minimap2 samtools bedtools fastqc multiqc\n",
    "#\n",
    "# pip commands to install all dependencies:\n",
    "#   pip install csv glob requests subprocess xmltodict sys lxml os urllib pyyaml xlrd\n",
    "#   pip install argparse pandas bioconda sra-tools entrez-direct\n",
    "#   pip install bowtie2 minimap2 samtools bedtools\n",
    "#   pip install fastqc multiqc\n",
    "#\n",
    "# Instruction:\n",
    "#       python EBI_SRA_Downloader.py -project [accession] [accession ... N]\n",
    "#                   Generate the study info, study detail, prep, and  sample\n",
    "#                   files for the entered EBI accession, and download the\n",
    "#                   FASTQ files.\n",
    "#               Optional flags:\n",
    "#                   -output [directory where files will be saved]\n",
    "#                   -mode [specifies which repository to use]                    \n",
    "#                   -prefix [list of prefixes for sample and prep info files]\n",
    "#                   --strategy [list of one or more library strategies to select]\n",
    "#                   --platforms [list of one or more sequencing platforms to select]\n",
    "#                   --validators [list of one or more yaml files to use in validating]\n",
    "#                   --no_seqs [skip downloading files]\n",
    "#                   --verbose          \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries used\n",
    "import csv\n",
    "import sys\n",
    "import glob\n",
    "import logging\n",
    "import requests\n",
    "import subprocess\n",
    "from xmltodict import parse\n",
    "from lxml import etree\n",
    "import os\n",
    "from os import path, makedirs, remove\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "from pandas import read_csv, DataFrame\n",
    "import numpy as np\n",
    "import re\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START: GENERIC CLEANUP METHODS\n",
    "\n",
    "def split_caps(cap_string):\n",
    "    \"\"\"Splits up a string based on capital letters\n",
    "\n",
    "    Metadata from SRA tends to include multiple entries destinguished by their\n",
    "    capitalization. This method splits up these strings and returns them\n",
    "    with underscore delimiting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cap_string : string\n",
    "        Thestring to be parsed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        underscore delimited string\n",
    "    \"\"\"\n",
    "    temp_list = []\n",
    "    temp_list = re.findall('[A-Z][^A-Z]*', cap_string)\n",
    "    if len(temp_list) > 0:\n",
    "        return \"_\".join(temp_list)\n",
    "    else:\n",
    "        return cap_string\n",
    "        \n",
    "def scrub_special_chars(input_string,custom_dict={}):\n",
    "    \"\"\"Removes special characters from a string\n",
    "\n",
    "    This method scrubs special characters from any input string and\n",
    "    replaces them with underscores for punctuation and spelled-out \n",
    "    strings for other characters. Users may supply a custom dictionary\n",
    "    which will be applied prior to the base rules to ensure that user\n",
    "    preference for renaming is respected, e.g. \"/\" may be converted to\n",
    "    '_or_' by the user to avoid conversion to '_per' by the default\n",
    "    dictionary. Returned strings are also compatible with Qiita column\n",
    "    requirements.   \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_string : string\n",
    "        The string to be scrubbed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        returns the scrubbed string\n",
    "    \"\"\"\n",
    "    \n",
    "    replace_dict ={\"__\":\"_\",\n",
    "                   \" \":\"_\",\n",
    "                   \"-\": \"_\",\n",
    "                   \"(\": \"_leftparen_\",\n",
    "                   \")\": \"_rightparen_\",\n",
    "                   \"/\": \"_per_\",\n",
    "                   \"-\":\"_\",\n",
    "                   \"|\":\"_bar_\",\n",
    "                   \"~\":\"_tilde_\",\n",
    "                   \"`\":\"_\",\n",
    "                   \"@\":\"_at_\",\n",
    "                   \"#\":\"_number_\",\n",
    "                   \"$\":\"dollar\",\n",
    "                   \"%\":\"_perc_\",\n",
    "                   \"^\":\"_carrot_\",\n",
    "                   \"&\":\"_and_\",\n",
    "                   \"*\":\"_star_\",\n",
    "                   \"+\":\"_plus\",\n",
    "                   \"=\":\"_equals\",\n",
    "                   \"\\\\\":\"_per_\",\n",
    "                   \"{\":\"_leftbracket_\",\n",
    "                   \"}\":\"_rightbracket_\",\n",
    "                   \"[\":\"_leftbracket_\",\n",
    "                   \"]\":\"_rightbracket_\",\n",
    "                   \"?\":\"_question\",\n",
    "                   \"<\":\"_less_\",\n",
    "                   \">\":\"_more_\",\n",
    "                   \",\":\"_\",\n",
    "                   \".\":\"_\",}\n",
    "    for k in custom_dict.keys():\n",
    "        input_string=input_string.replace(k,custom_dict[k])\n",
    "    for k in replace_dict.keys():\n",
    "        input_string=input_string.replace(k,replace_dict[k])\n",
    "    \n",
    "    return input_string\n",
    "\n",
    "def sub_sample_study(sample_df,max,rand_sample):\n",
    "    \"\"\"Subsamples a dataframe for the specified number of samples\n",
    "\n",
    "    Simple helper function to take a dataframe and return the top or\n",
    "    a random subset based on the number requested\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_df : pd.DataFrame\n",
    "        The dataframe to be subsampled\n",
    "    max: int\n",
    "        The number of samples to take\n",
    "    rand_sample: boolean\n",
    "        Whether to perform a random subsample\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sample_df\n",
    "        subsampled dataframe\n",
    "    \"\"\"\n",
    "    if rand_sample:\n",
    "        return sample_df.sample(max)\n",
    "    else:\n",
    "        return sample_df.head(max)\n",
    "\n",
    "### END: GENERIC CLEANUP METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START: NORMALIZATION STAGING METHODS\n",
    "\n",
    "def qiimp_parser(filename):\n",
    "    \"\"\"Parses Qiimp-format yaml or xlsx files to obtain a dictionary of\n",
    "    rules for metadata normalization\n",
    "\n",
    "    This method will accept an input xlsx from Qiimp (qiita.ucsd.edu/qiimp)\n",
    "    with a yaml formatted set of rules in cell A1 of the Sheet metadata_schema,\n",
    "    or a yaml/yml file with the same format which may be created by extracting\n",
    "    the rules from such a Qiimp template into a new yaml/yml file. Throws errors\n",
    "    if unexpected .xlsx formatted file or corrupted yaml is found.\n",
    "    \n",
    "    N.B. this method can be used to parse any yaml/yml file into a Python dict.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : string\n",
    "        The filename of the Qiimp .xlsx or yml to be parsed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parse_yml\n",
    "        returns the scrubbed string\n",
    "    \"\"\"\n",
    "    ext=Path(filename).suffix\n",
    "    parsed_yml={}\n",
    "    if ext == '.xlsx':\n",
    "        try:\n",
    "            temp_yml=pd.read_excel(filename,sheet_name='metadata_schema',header=None) #assume QIIMP-style excel\n",
    "            parsed_yml = yaml.load(temp_yml.at[0,0])\n",
    "        except:\n",
    "            logger.warning(\"Invalid .xlsx file. Please ensure file is from QIIMP or contains a compliant yaml \" + \n",
    "                          \"in cell A1 of the sheet labelled 'metadata_schema'.\")\n",
    "    elif ext == '.yml' or ext == '.yaml':\n",
    "        try:\n",
    "            with open(filename) as file:\n",
    "            # The FullLoader parameter handles the conversion from YAML\n",
    "            # scalar values to Python dictionary format\n",
    "                parsed_yml=yaml.load(file, Loader=yaml.FullLoader)\n",
    "                if DEBUG: logger.info(parsed_yml)\n",
    "        except:\n",
    "            logger.warning(\"Could not load yaml from \" + filename + \"Contents:\\n\")\n",
    "            logger.warning(subprocess.run(['cat',filename]))\n",
    "    else:\n",
    "        logger.warning(\"Invalid file extension for yaml parsing: \" + str(ext))\n",
    "\n",
    "    if len(parsed_yml) == 0:\n",
    "        logger.warning(\"The file \" + filename +\" contains no yaml data. Please check contents and try again.\")\n",
    "\n",
    "    return parsed_yml\n",
    "\n",
    "def set_yaml_validators(validators,fields=['scientific_name']):\n",
    "    \"\"\"Used to set up dictionary of validators for metadata normalization\n",
    "    \n",
    "    This method takes a set of supplied yaml, yml or xlsx files and looks\n",
    "    for a scientific_name field to serve as the key for a dictionary of\n",
    "    validators used to normalize the metadata. All files parsed are expected to\n",
    "    be in Qiimp-format and validators that do not contain a default for \n",
    "    scientific_name will be omitted. Users may supply a custom field to look for\n",
    "    instead of scientific_name for key creation.\n",
    "    \n",
    "    #TODO: this method currently cannot differentiate between sufficiently vague\n",
    "    scientific names, e.g. human metagenome for multiple body sites\n",
    "    skin, vaginal, etc.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    validators : list\n",
    "        The filenames of the Qiimp .xlsx or yml to be stored\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    yaml_dict\n",
    "        returns the dictionary of validators\n",
    "    \"\"\"\n",
    "    yaml_dict={}\n",
    "    for v in validators:\n",
    "        new_yaml={}\n",
    "        #try:\n",
    "        new_yaml = qiimp_parser(v)\n",
    "        #except:\n",
    "        #    logger.warning(\"Could not open yaml file \" + v + \" Please ensure the file exists and is a valid yaml file.\")\n",
    "\n",
    "        for field in fields:\n",
    "            if field not in new_yaml.keys():\n",
    "                logger.warning(\"Invalid validator yaml. Please ensure a default value is provided for '\" + field +\n",
    "                      \"'. Available keys in file: \" + str(new_yaml.keys()))\n",
    "            elif 'default' not in new_yaml[field].keys():\n",
    "                logger.warning(\"Invalid validator yaml. Please ensure a default value is provided for '\" + field +\n",
    "                      \"'. Available keys in : \" + field + str(new_yaml[field].keys()))\n",
    "            else:\n",
    "                yaml_dict[new_yaml[field]['default']] = v\n",
    "\n",
    "    return yaml_dict\n",
    "\n",
    "def set_empo_normalizers(sample_type_map):\n",
    "    \"\"\"Reads in an mapping file that for EMPO-related comparisons\n",
    "\n",
    "    This method will accept an input tab or comma-delimited file that contains\n",
    "    the column sample_type which will be used to normalize sample_type temrs\n",
    "    and EMPO fields to support better metaanalysis across public studies. Returns\n",
    "    a pandas DataFrame for use in normalization\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_type_map : string\n",
    "        The filename of the mapping file to be read\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    empo_type_df\n",
    "        returns the DataFrame for mapping\n",
    "    \"\"\"\n",
    "    empo_type_df =pd.DataFrame()\n",
    "    empo_type_df=pd.read_csv(sample_type_map, sep='\\t', dtype=str)\n",
    "    if 'sample_type' not in empo_type_df: #try to load again as a csv\n",
    "        empo_type_df = pd.read_csv(sample_type_map)\n",
    "\n",
    "    if 'sample_type' not in empo_type_df:\n",
    "        logger.warning(\"sample_type not found in sample type mapping file: \" + sample_type_map + \"\\nAvailable columns: \" + empo_type_df.columns)\n",
    "    else:\n",
    "        empo_type_df=empo_type_df.set_index('sample_type')\n",
    "        logger.info(\"Sample type mapping file found. Adding normalization columns: \" + empo_type_df.columns)\n",
    "\n",
    "    return empo_type_df\n",
    "\n",
    "\n",
    "### END: NORMALIZATION STAGING METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START: NORMALIZATION METHODS\n",
    "def set_prep_type(expt_df,row):\n",
    "    \"\"\"Maps EBI library strategies to Qiita prep types\n",
    "\n",
    "    Method to map  EBI library strategies to Qiita prep types. For amplicon\n",
    "    and 'other' types, also looks for target_gene to set prep type. Any\n",
    "    ambigious prep type is labeled as such and this dictionary can be updated\n",
    "    as new prep types are supported. Also attempts to account for common typos\n",
    "    and format changes for target_gene.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    expt_df : pd.DataFrame\n",
    "        The dataframe of experiment data from EBI to be normalized\n",
    "    row: index value\n",
    "        The specific line of the dataframe to look up\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    library_strat_to_qiita_dict[ebi_strat] OR\n",
    "    amplicon_dict[tg] OR\n",
    "    'AMBIGUOUS': string\n",
    "        Qiita-normalized prep type term\n",
    "    \"\"\"\n",
    "    amplicon_list = ['AMPLICON','OTHER']\n",
    "    amplicon_dict= {'16S rRNA':'16S',\n",
    "                    '16S':'16S',\n",
    "                    '16S rDNA':'16S',\n",
    "                    '16S RNA':'16S',\n",
    "                    '16S DNA':'16S',\n",
    "                    '16S ':'16S',\n",
    "                    'ITS':'ITS',\n",
    "                    'ITS1':'ITS',\n",
    "                    'ITS2':'ITS',\n",
    "                    'ITS ':'ITS',\n",
    "                    '18S rRNA':'18S',\n",
    "                    '18S rDNA':'18S',\n",
    "                    '18S':'18S',\n",
    "                    '18S RNA':'18S',\n",
    "                    '18S DNA':'18S',\n",
    "                    '18S ':'18S',\n",
    "                    }\n",
    "    library_strat_to_qiita_dict={'POOLCLONE':'Metagenomic',\n",
    "        'CLONE':'Metagenomic',\n",
    "        'CLONEEND':'AMBIGIOUS',\n",
    "        'WGS':'Metagenomic',\n",
    "        'WGA':'Metagenomic',\n",
    "        'WCS':'AMBIGIOUS',\n",
    "        'WXS':'Metatranscriptomic',\n",
    "        'ChIP-Seq':'Metagenomic',\n",
    "        'RNA-Seq':'Metatranscriptomic',\n",
    "        'MRE-Seq':'AMBIGIOUS',\n",
    "        'MeDIP-Seq':'AMBIGIOUS',\n",
    "        'MBD-Seq':'AMBIGIOUS',\n",
    "        'MNase-Seq':'AMBIGIOUS',\n",
    "        'DNase-Hypersensitivity':'AMBIGIOUS',\n",
    "        'Bisulfite-Seq':'Metgenomic',\n",
    "        'EST':'AMBIGIOUS',\n",
    "        'FL-cDNA':'AMBIGIOUS',\n",
    "        'miRNA-Seq':'Metatranscriptomic',\n",
    "        'ncRNA-Seq':'Metatranscriptomic',\n",
    "        'FINISHING':'AMBIGIOUS',\n",
    "        'CTS':'AMBIGIOUS',\n",
    "        'Tn-Seq':'AMBIGIOUS',\n",
    "        'VALIDATION':'AMBIGIOUS',\n",
    "        'FAIRE-seq':'AMBIGIOUS',\n",
    "        'SELEX':'AMBIGIOUS',\n",
    "        'RIP-Seq':'AMBIGIOUS',\n",
    "        'ChIA-PET':'AMBIGIOUS',\n",
    "        'RAD-Seq':'AMBIGIOUS',}\n",
    "    ebi_strat = expt_df.at[row,'library_strategy']\n",
    "\n",
    "    if ebi_strat not in amplicon_list:\n",
    "        try:\n",
    "            return library_strat_to_qiita_dict[ebi_strat]\n",
    "        except:\n",
    "            logger.warning(ebi_strat + \" not found in EBI-Qiita library strategy mapping. Setting to 'AMBIGUOUS'.\")\n",
    "            return 'AMBIGIOUS'\n",
    "    else:\n",
    "        #since this is amplicon data, there should be a target gene, if not return AMBIGUOUS\n",
    "        try:\n",
    "            tg=expt_df.at[row,'target_gene']\n",
    "            return amplicon_dict[tg]\n",
    "        except:\n",
    "            logger.warning(ebi_strat + \" not found in amplicon resolution mapping. Setting to 'AMBIGUOUS'.\")\n",
    "            return 'AMBIGIOUS'\n",
    "\n",
    "def normalize_types(md,mapping):\n",
    "    \"\"\"Maps EBI library strategies to Qiita prep types\n",
    "\n",
    "    Method to map  EBI library strategies to Qiita prep types. For amplicon\n",
    "    and 'other' types, also looks for target_gene to set prep type. Any\n",
    "    ambigious prep type is labeled as such and this dictionary can be updated\n",
    "    as new prep types are supported. Also attempts to account for common typos\n",
    "    and format changes for target_gene.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    expt_df : pd.DataFrame\n",
    "        The dataframe of experiment data from EBI to be normalized\n",
    "    row: index value\n",
    "        The specific line of the dataframe to look up\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    library_strat_to_qiita_dict[ebi_strat] OR\n",
    "    amplicon_dict[tg] OR\n",
    "    'AMBIGUOUS': string\n",
    "        Qiita-normalized prep type term\n",
    "    \"\"\"\n",
    "    #small modifications from Daniel McDonald normalization code\n",
    "    simple_sample_type = mapping['simple_sample_type'].to_dict()\n",
    "    empo_1 = mapping['empo_1'].to_dict()\n",
    "    empo_2 = mapping['empo_2'].to_dict()\n",
    "    empo_3 = mapping['empo_3'].to_dict()\n",
    "\n",
    "    qsst = [simple_sample_type.get(v) for v in md['sample_type']]\n",
    "    qemp1 = [empo_1.get(v) for v in md['sample_type']]\n",
    "    qemp2 = [empo_2.get(v) for v in md['sample_type']]\n",
    "    qemp3 = [empo_3.get(v) for v in md['sample_type']]\n",
    "\n",
    "    md['qiita_empo_1'] = qemp1\n",
    "    md['qiita_empo_2'] = qemp2\n",
    "    md['qiita_empo_3'] = qemp3\n",
    "    md['simple_sample_type'] = qsst\n",
    "    \n",
    "    #to check for empo values, need to ensure columns are there to avoid error\n",
    "    empo_cols=['empo_1','empo_2','empo_3']\n",
    "    for e in empo_cols:\n",
    "        if e not in md.columns:\n",
    "            md[e]= np.nan\n",
    "    # if there is an existing empo value, then let's prefer it\n",
    "    emp1 = md[~md['empo_1'].isnull()]\n",
    "    emp2 = md[~md['empo_2'].isnull()]\n",
    "    emp3 = md[~md['empo_3'].isnull()]\n",
    "    md.loc[emp1.index, 'qiita_empo_1'] = emp1['empo_1']\n",
    "    md.loc[emp2.index, 'qiita_empo_2'] = emp1['empo_2']\n",
    "    md.loc[emp3.index, 'qiita_empo_3'] = emp1['empo_3']\n",
    "\n",
    "    return md\n",
    "\n",
    "def validate_samples(raw_df,sample_type_col,yaml_validator_dict,prefix):\n",
    "    \"\"\"Applies yaml validation rules to metadata\n",
    "\n",
    "    #TODO fill in description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_df: pd.DataFrame\n",
    "        un-normalized/validated dataframe of metadata    \n",
    "    sample_type_col : string\n",
    "        column for identifying sample type \n",
    "    yaml_validator_dict : dict\n",
    "        dictionary of Qiimp-style validators to use for normalization\n",
    "    prefix: string\n",
    "        prefix to assign to validator error log, includes path\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    valid_df: pd.DataFrame\n",
    "       Validated dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialize variables\n",
    "    st_list = []\n",
    "    msg = ''\n",
    "    validator_yaml={}\n",
    "\n",
    "    for st in raw_df[sample_type_col].unique():\n",
    "        df_to_validate = raw_df[raw_df[sample_type_col]==st]\n",
    "        if force:\n",
    "            validator_yaml = qiimp_parser(yaml_validator_dict[0])\n",
    "            if 'scientific_name' in df_to_validate.keys():\n",
    "                df_to_validate=df_to_validate.rename({'scientific_name':'orig_scientific_name'},axis=1)\n",
    "        else:\n",
    "            if st not in yaml_validator_dict.keys():\n",
    "                logger.warning(\"No yaml file for validating \" + st + \" You may provide one or more custom files \" +\n",
    "                  \" using the --validators flag.\")\n",
    "            else:\n",
    "                validator_yaml = qiimp_parser(yaml_validator_dict[st])\n",
    "\n",
    "        for k in validator_yaml.keys():\n",
    "            if k not in df_to_validate.columns:\n",
    "                msg= msg + k + ' not found in metadata.\\n' \n",
    "                try:\n",
    "                    df_to_validate[k]= validator_yaml[k]['default']\n",
    "                    msg = msg + \"Setting \" + k + \" to \" + validator_yaml[k]['default'] + \" for \" + st + \" samples\\n\"                     \n",
    "                except:\n",
    "                    df_to_validate[k]= 'not provided'\n",
    "                    msg = msg + k + \" has no default in yaml template. Encoding as 'not provided'\\n\"\n",
    "            else:\n",
    "                #construct rules\n",
    "                uniq = df_to_validate[k].unique()\n",
    "                allowed_list = []\n",
    "                min_value= ''\n",
    "                max_value = ''\n",
    "                min_value_excl= ''\n",
    "                max_value_excl = ''\n",
    "                if 'anyof' in validator_yaml[k].keys():\n",
    "                    anyof_list = validator_yaml[k]['anyof']            \n",
    "                    for r in anyof_list:\n",
    "                        if r['type'] == 'string':\n",
    "                            for a in r['allowed']:\n",
    "                                allowed_list.append(a)\n",
    "                        elif r['type'] == 'number':\n",
    "                            if 'min' in r.keys():\n",
    "                                min_value = r['min']\n",
    "                            if 'max' in r.keys():\n",
    "                                max_value = r['max']\n",
    "                            if 'min_exclusive' in r.keys():\n",
    "                                min_value = r['min_exclusive']\n",
    "                            if 'max_exclusive' in r.keys():\n",
    "                                max_value = r['max_exclusive']\n",
    "                elif validator_yaml[k]['type'] in validator_yaml[k].keys():\n",
    "                    if validator_yaml[k]['type']== 'string':\n",
    "                        allowed_list=validator_yaml[k]['allowed']\n",
    "                    if validator_yaml[k]['type'] == 'number' or validator_yaml[k]['type'] =='integer':\n",
    "                        if 'min' in validator_yaml[k].keys():\n",
    "                            min_value = validator_yaml[k]['min']\n",
    "                        if 'max' in validator_yaml[k].keys():\n",
    "                            max_value = validator_yaml[k]['max']\n",
    "                        if 'min_exclusive' in validator_yaml[k].keys():\n",
    "                            min_value_excl = validator_yaml[k]['min']\n",
    "                        if 'max_exclusive' in validator_yaml[k].keys():\n",
    "                            max_value_excl = validator_yaml[k]['max']\n",
    "\n",
    "                #alert user of issues\n",
    "                for u in uniq.astype(str):\n",
    "                    if not u.isnumeric():\n",
    "                        if u not in allowed_list and len(allowed_list) > 0:\n",
    "                            msg = msg + \"Warning \" + u + \" found in column \" + k + \" but not allowed per Qiimp template.\" +\\\n",
    "                            \"valid values: \" + str(allowed_list) + \"\\n\"          \n",
    "                    else:\n",
    "                        if u not in allowed_list: #assume it's actually a number\n",
    "                            if min_value != '' and u < min_value:\n",
    "                                msg = msg + \"Warning \" + u + \" found in column \" + k + \" but less than min value per yaml: \" +\\\n",
    "                                str(min_value) + \"\\n\"\n",
    "                            if max_value != '' and u > max_value:\n",
    "                                msg = msg + \"Warning \" + u + \" found in column \" + k + \" but more than max value per yaml: \" +\\\n",
    "                                     str(max_value) + \"\\n\"\n",
    "                            if min_value_excl != '' and u <= min_value_excl:\n",
    "                                msg = msg + \"Warning \" + u + \" found in column \" + k + \" but less than min value per yaml: \" +\\\n",
    "                                     str(min_value_excl) + \"\\n\"\n",
    "                            if max_value_excl != '' and u >= max_value_excl:    \n",
    "                                lmsg = msg + \"Warning \" + u + \" found in column \" + k + \" but not allowed per yaml: \" +\\\n",
    "                                str(max_value_excl) + \"\\n\"\n",
    "        st_list.append(df_to_validate)\n",
    "        if len(msg) > 0:\n",
    "            logger.warning(\"Errors found during validation:\")\n",
    "            logger.warning(msg)\n",
    "            if DEBUG:\n",
    "                st_log_name=st.replace(' ','_')\n",
    "                valid_log_filename = prefix + \"_\" + st_log_name + '_validation_errors.log'\n",
    "                errors = open(valid_log_filename, \"w\")\n",
    "                n = errors.write(msg)\n",
    "                errors.close()\n",
    "                logger.warning(\"Validation errors written to \" + valid_log_filename)\n",
    "    valid_df = pd.concat(st_list)\n",
    "    return valid_df\n",
    "\n",
    "### END: NORMALIZATION METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START: EBI/ENA AND NCBI/SRA DATA RETRIEVAL METHODS\n",
    "def get_study_details(study_accession,mode='ebi',prefix=''):\n",
    "    \"\"\"Retrieve study information including list of samples\n",
    "\n",
    "    #TODO fill in description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    study_accession:string\n",
    "        project or study name from EBI/ENA or NCBI/SRA\n",
    "    mode : string\n",
    "        'ebi' or 'sra'; repo to use for data retrieval\n",
    "    prefix: string\n",
    "        prefix to assign to validator error log, includes path\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    study_df: pd.DataFrame\n",
    "       dataframe of study information as provided by the chosen repository\n",
    "    \"\"\"\n",
    "    if mode == 'ebi':\n",
    "        studyUrl = \"http://www.ebi.ac.uk/ena/data/view/\" + study_accession \\\n",
    "                    + \"&display=xml\"\n",
    "        try:\n",
    "            response = requests.get(studyUrl)\n",
    "            xml_dict=parse(response.content)\n",
    "        except:\n",
    "            logger.error(\"Could not obtain study information for \" + study_accession + \" at URL: \" + studyUrl\n",
    "                             + \" . Please check connection and study/project name and try again.\")\n",
    "\n",
    "        if 'STUDY_SET' in xml_dict.keys():\n",
    "            logger.info(study_accession + \" is study ID. Writing config file\")\n",
    "            id_tuple = write_config_file(xml_dict,prefix,mode)\n",
    "        elif 'PROJECT_SET' in xml_dict.keys():\n",
    "            try:\n",
    "                secondary_accession = xml_dict['PROJECT_SET']['PROJECT']['IDENTIFIERS']['SECONDARY_ID']\n",
    "                secondaryUrl = \"http://www.ebi.ac.uk/ena/data/view/\" + secondary_accession + \"&display=xml\"\n",
    "                logger.warning(study_accession + \" is project ID. Retrieved secondary ID: \" + secondary_accession + \" Writing config file.\")\n",
    "                logger.info(\"Alternate url\" + secondaryUrl)\n",
    "                response2 = requests.get(secondaryUrl)\n",
    "                xml_study_dict=parse(response2.content)\n",
    "                id_tuple = write_config_file(xml_study_dict,prefix,mode,xml_dict)\n",
    "\n",
    "            except:\n",
    "                logger.error(\"No matching study ID found for project \" + study_accession)\n",
    "        else:\n",
    "            logger.warning(\"No study information found for \" + study_accession)\n",
    "\n",
    "        host = \"http://www.ebi.ac.uk/ena/data/warehouse/filereport?accession=\"\n",
    "        read_type = \"&result=read_run&\"\n",
    "        fields = \"library_name,secondary_sample_accession,run_accession,\" + \\\n",
    "                 \"experiment_accession,fastq_ftp,library_source,\" + \\\n",
    "                 \"instrument_platform,submitted_format,library_strategy,\" +\\\n",
    "                 \"library_layout,tax_id,scientific_name,instrument_model,\" + \\\n",
    "                \"library_selection,center_name,experiment_title,\" +\\\n",
    "                \"study_title,study_alias,experiment_alias,sample_alias,sample_title\"\n",
    "        url = ''.join([host, study_accession, read_type, \"fields=\", fields])\n",
    "        if DEBUG: logger.info(url)\n",
    "        study_df = pd.read_csv(url,sep='\\t')\n",
    "        study_df.dropna(axis=1,how='all',inplace=True)\n",
    "\n",
    "        #add primary (project) and secondary (study) ID to dataframe\n",
    "        study_df['primary_ebi_id']=id_tuple[0]\n",
    "        study_df['secondary_ebi_id']=id_tuple[1]\n",
    "        study_df['ebi_import']='TRUE'\n",
    "\n",
    "    elif mode == 'sra':\n",
    "        p1 = subprocess.Popen(['esearch', '-db', 'sra', '-query', study_accession],\n",
    "                   stdout=subprocess.PIPE)\n",
    "        for i in p1.stdout:\n",
    "            if \"<Count>0</Count>\" in i.decode(\"utf-8\"):\n",
    "                p1.stdout.close()\n",
    "                raise Exception(study_accession + \" is not a valid SRA study ID\")\n",
    "\n",
    "        p1 = subprocess.Popen(['esearch', '-db', 'sra', '-query', study_accession],\n",
    "                           stdout=subprocess.PIPE)\n",
    "        p2 = subprocess.Popen(['efetch', '-format', 'native'], stdin=p1.stdout,\n",
    "                           stdout=subprocess.PIPE)\n",
    "        p1.stdout.close()\n",
    "\n",
    "        for i in p2.stdout:\n",
    "            line = i.decode(\"utf-8\")\n",
    "            start = line.find(\"<STUDY \")\n",
    "            if(start != -1):\n",
    "                end = line.find(\"</STUDY>\")\n",
    "                info = line[start: end+8]\n",
    "                doc = parse(info)\n",
    "                break\n",
    "        p2.stdout.close()\n",
    "        write_config_file(doc,study_accession,mode)\n",
    "        \n",
    "        p1 = subprocess.Popen(['esearch', '-db', 'sra', '-query', study_accession],\n",
    "                          stdout=subprocess.PIPE)\n",
    "        p2 = subprocess.Popen(['efetch', '-format', 'runinfo'], stdin=p1.stdout,\n",
    "                          stdout=subprocess.PIPE)\n",
    "        count=0\n",
    "        headers=[]\n",
    "        for i in p2.stdout:\n",
    "            if count == 0:\n",
    "                sra_headers = i.decode(\"utf-8\").replace('\\n','').split(',')\n",
    "                for h in sra_headers:\n",
    "                    headers.append(scrub_special_chars(split_caps(h).lower(),custom_dict={'i_d':'id',\n",
    "                                                                                    's_r_a':'sra',\n",
    "                                                                                    'e_b_i':'ebi',\n",
    "                                                                                    's_r_r':'srr',\n",
    "                                                                                    'm_b':'mb'}))\n",
    "                    study_df=pd.DataFrame({},columns=headers)\n",
    "            else:\n",
    "                tmp=str(i.decode(\"utf-8\"))\n",
    "\n",
    "                if len(tmp) > 1:\n",
    "                    a_series = pd.Series(list(csv.reader(tmp.splitlines(),delimiter=','))[0], index = study_df.columns)\n",
    "                    study_df = study_df.append(a_series, ignore_index=True)\n",
    "            count += 1\n",
    "    else:\n",
    "        raise Exception(mode + \" is not a valid repository.\")\n",
    "\n",
    "    return study_df\n",
    "\n",
    "\n",
    "#now feed in study_df above\n",
    "def get_sample_info(input_df,mode='ebi',plat=[],strat=[],validator_files={},prefix='',names=[],\n",
    "                    src=[],empo_mapping=pd.DataFrame(),sample_type_column='scientific_name'):\n",
    "    \"\"\"Retrieve information for each sample including experiment/prep information\n",
    "\n",
    "    #TODO fill in description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df : pd.DataFrame\n",
    "        dataframe of samples for retreival\n",
    "    mode : string\n",
    "        'ebi' or 'sra'; repo to use for data retrieval\n",
    "    plat :  list\n",
    "        list of valid plaforms to filter for\n",
    "    strat: list\n",
    "        list of valid library strategies to filter for\n",
    "    validator_files: dict\n",
    "        dictionary of available validation files for metadata normalization\n",
    "    prefix: string\n",
    "        prefix to assign to validator error log, includes path\n",
    "    names : list\n",
    "        list of scientific names to filter for\n",
    "    src : list\n",
    "        list of library sources to filter for\n",
    "    empo_mapping : pd.DataFrame()\n",
    "        datagframe of loaded EMPO and/or sample_type mappings\n",
    "    sample_type_column : string\n",
    "        column for identifying sample type \n",
    "\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    study_df: pd.DataFrame\n",
    "       dataframe of study information as provided by the chosen repository\n",
    "    \"\"\"\n",
    "    \n",
    "    #need to provide way to list additional prep_info columns, now function will return tuple\n",
    "    add_prep_cols=[]\n",
    "\n",
    "    #check to see if the .part file already exists:\n",
    "    pre_validation_file = prefix + \"_unvalidated_sample_info.part\"\n",
    "    if not path.isfile(pre_validation_file):\n",
    "        if mode =='ebi':\n",
    "            identifier = 'secondary_sample_accession'\n",
    "            run_accession = 'run_accession'\n",
    "            input_df['platform']=input_df['instrument_platform']\n",
    "        elif mode == 'sra':\n",
    "            identifier = 'sample'\n",
    "            run_accession = 'run'\n",
    "            input_df['instrument_model']=input_df['model']\n",
    "        else:\n",
    "            raise Exception(mode + \" is not a valid repository.\")\n",
    "\n",
    "        #Note: for now this loop just uses the data in EBI since it is mirrored with NCBI\n",
    "        sample_info_list=[]\n",
    "        sample_count_dict = {}\n",
    "        prep_df_dict={}\n",
    "\n",
    "        #apply filters for platforms and strategies\n",
    "        except_msg=''\n",
    "        if len(plat) > 0:\n",
    "            except_msg = except_msg + \"Selected Platforms: \" + str(plat) + \"\\n Available Platforms:\" +\\\n",
    "                        str(input_df['platform'].str.lower().unique()) + \"\\n\"\n",
    "            input_df = input_df[input_df['platform'].str.lower().isin(plat)]\n",
    "\n",
    "        if len(strat) > 0:\n",
    "            except_msg = except_msg + \"Selected Strategies: \" + str(strat) + \"\\n Available Strategies:\" +\\\n",
    "                        str(input_df['library_strategy'].str.lower().unique()) + \"\\n\"\n",
    "            input_df = input_df[input_df['library_strategy'].str.lower().isin(strat)]\n",
    "\n",
    "        if len(names) > 0:\n",
    "            except_msg = except_msg + \"Selected Scientific Names: \" + str(names) + \"\\n Available Scientific Names:\" +\\\n",
    "                        str(input_df['scientific_name'].str.lower().unique()) + \"\\n\"\n",
    "            input_df = input_df[input_df['scientific_name'].str.lower().isin(names)]\n",
    "\n",
    "        if len(src) > 0:\n",
    "            except_msg = except_msg + \"Selected Library Sources: \" + str(src) + \"\\n Available Library Sources:\" +\\\n",
    "                        str(input_df['library_source'].str.lower().unique()) + \"\\n\"\n",
    "            input_df = input_df[input_df['library_source'].str.lower().isin(src)]\n",
    "\n",
    "        if len(input_df) == 0:\n",
    "            raise Exception(\"No files after selection criteria:\\n\" + except_msg)\n",
    "\n",
    "        #need to catch common issue where identifier is identical for unique samples\n",
    "        #for now assume that in this case, library_name will be unique, and if it isn't combine sample and run names\n",
    "        if len(input_df) > 1 and input_df[identifier].nunique() == 1:\n",
    "            if input_df['library_name'].nunique() != 1:\n",
    "                 #print(str(len(_input_df)) + \" is length and input_df[identifier].nunique() = \" + str(input_df[identifier].nunique()))\n",
    "                 input_df['sample_name']=input_df['library_name'].apply(lambda x: scrub_special_chars(x).replace('_','.'))\n",
    "            else:\n",
    "                 input_df['sample_name']=input_df[identifier]+ '.' + input_df[run_accession]\n",
    "        else:\n",
    "            input_df['sample_name']=input_df[identifier]\n",
    "\n",
    "        input_df['run_prefix']=input_df[run_accession]\n",
    "\n",
    "        for index, row in input_df.iterrows():\n",
    "            sample_accession = row[identifier]\n",
    "#            ebi_lib_strat = row['library_strategy']\n",
    "            sample_name = row['sample_name']\n",
    "            expt_accession = row['experiment_accession']\n",
    "\n",
    "            sampleUrl = \"http://www.ebi.ac.uk/ena/data/view/\" + sample_accession \\\n",
    "                      + \"&display=xml\"\n",
    "            if DEBUG: logger.info(sampleUrl)\n",
    "\n",
    "            sample_response = requests.get(sampleUrl)\n",
    "            sample_xml_dict=parse(sample_response.content)\n",
    "\n",
    "            exptUrl = \"http://www.ebi.ac.uk/ena/data/view/\" + expt_accession \\\n",
    "                      + \"&display=xml\"\n",
    "            if DEBUG: logger.info(exptUrl) \n",
    "            expt_response = requests.get(exptUrl)\n",
    "            expt_xml_dict=parse(expt_response.content)\n",
    "\n",
    "\n",
    "            if 'EXPERIMENT_SET' not in expt_xml_dict.keys():\n",
    "                logger.warning('No experimental metadata found for sample named: ' + sample_accession + ' with experiment_accession ' + expt_accession + ', omitting.')\n",
    "            else:\n",
    "                if 'SAMPLE_SET' not in sample_xml_dict.keys():\n",
    "                    logger.warning('No metadata found for sample named: ' + sample_accession + ' omitting.')\n",
    "                else:\n",
    "                    input_df.at[index,'experiment_title_specific']=expt_xml_dict['EXPERIMENT_SET']['EXPERIMENT']['TITLE']\n",
    "                    try:\n",
    "                        ea=expt_xml_dict['EXPERIMENT_SET']['EXPERIMENT']['EXPERIMENT_ATTRIBUTES']['EXPERIMENT_ATTRIBUTE']\n",
    "                        for e in ea:\n",
    "                            col = scrub_special_chars(e['TAG']).lower()\n",
    "                            #print(col)\n",
    "                            if col not in add_prep_cols:\n",
    "                                add_prep_cols.append(col)\n",
    "                            try:\n",
    "                                input_df.at[index,col]=e['VALUE']\n",
    "                            except:\n",
    "                                logger.warning('No value found for experiment attribute: ' + col + '. Setting to \"not provided\".') \n",
    "                    except:\n",
    "                        logger.warning(\"No experiment attributes found for \" + expt_accession + \" corresponding to sample \" + sample_accession)\n",
    "                    #need to get prep_type and convert based on dictionary and additional information, e.g.target_gene for amplicon\n",
    "                    prep_type = set_prep_type(input_df, index)\n",
    "\n",
    "                    if prep_type not in sample_count_dict.keys() :\n",
    "                        sample_count_dict[prep_type]= {sample_name:0}\n",
    "                    elif sample_name not in sample_count_dict[prep_type].keys():\n",
    "                        sample_count_dict[prep_type][sample_name]= 0\n",
    "                    else:\n",
    "                        sample_count_dict[prep_type][sample_name] = sample_count_dict[prep_type][sample_name] + 1\n",
    "\n",
    "                    #now add sample specific information\n",
    "                    input_df.at[index,'sample_title_specific']=sample_xml_dict['SAMPLE_SET']['SAMPLE']['TITLE']\n",
    "\n",
    "                    sn=sample_xml_dict['SAMPLE_SET']['SAMPLE']['SAMPLE_NAME']\n",
    "                    for s in sn.keys():\n",
    "                        col = scrub_special_chars(s).lower()\n",
    "                        #print(col)\n",
    "                        input_df.at[index,col]=sn[s]\n",
    "\n",
    "                    sa=sample_xml_dict['SAMPLE_SET']['SAMPLE']['SAMPLE_ATTRIBUTES']['SAMPLE_ATTRIBUTE']\n",
    "                    for s in sa:\n",
    "                        col = scrub_special_chars(s['TAG']).lower()\n",
    "                        #print(col)\n",
    "                        try:\n",
    "                            input_df.a[index,col]=s['VALUE']\n",
    "                        except:\n",
    "                            logger.warning('No value found for sample attribute: ' + col + '. Setting to \"not provided\".')\n",
    "                        input_df.at[index,col]='not provided'\n",
    "                    #this sets the key used for splitting the files into prep_info templates\n",
    "                    input_df.at[index,'prep_file']=prep_type + '_' + str(sample_count_dict[prep_type][sample_name])\n",
    "\n",
    "\n",
    "        #the loops above takes the most time so write out a placeholder file for faster re-running if interrupted\n",
    "        input_df.to_csv(pre_validation_file,sep='\\t',index=False)\n",
    "    else:\n",
    "        input_df = pd.read_csv(pre_validation_file,sep='\\t',dtype=str)\n",
    "\n",
    "    #start by normalizing sample types and EMPO fields if sample_type is present\n",
    "    if len(empo_mapping) > 0:\n",
    "        if 'sample_type' in input_df.columns:\n",
    "            input_df = normalize_types(input_df,empo_mapping)\n",
    "        else:\n",
    "            logger.warning(\"'sample_type' not found in metadata. Skipping EMPO normalization.\")\n",
    "\n",
    "    output_df=validate_samples(input_df,sample_type_column,validator_files,prefix)\n",
    "\n",
    "    #tidy output before returning\n",
    "    output_df.columns = [scrub_special_chars(col).lower() for col in output_df.columns]\n",
    "\n",
    "    return output_df, add_prep_cols\n",
    "\n",
    "\n",
    "def fetch_sequencing_data(download_df,output_dir=\"./\",mode='ebi',host_deplete=False,host_db='/databases/minimap2/human-phix-db.mmi',cpus=4,hd_method='minimap2',qc=True,quality=False):\n",
    "    \"\"\"Fetch fastq files and run quality or host depletion per file (pair) as requested\n",
    "    \n",
    "    #TODO: complete description\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    download_df: pd.DataFrame\n",
    "        dataframe with sample and path information for downloading\n",
    "    output_dir : string\n",
    "        the path to save the output files\n",
    "    mode : string\n",
    "        'ebi' or 'sra'; repo to use for data retrieval\n",
    "    host_deplete: boolean\n",
    "        whether to run host depletion on the files as they are downloaded\n",
    "    host_db : string\n",
    "        path to the databased to be used for filtering\n",
    "    cpus : int\n",
    "        number of threads to use for fastp and fastq\n",
    "    hd_method : string\n",
    "        method for host depletion\n",
    "    qc : boolean\n",
    "        whether to run fastqc on output\n",
    "    quality: boolean\n",
    "        whether to run fastp on downloaded files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    valid_df : pd.DataFrame\n",
    "        dataframe of samples that were successfully downloaded\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Downloading the fastqs\")\n",
    "    # Download the fastqs\n",
    "    \n",
    "    for index, row in download_df.iterrows():\n",
    "        sequencer = row['instrument_model']\n",
    "        failed_list=[]\n",
    "        hd_file_list =[]\n",
    "        if mode == 'ebi':\n",
    "            try:\n",
    "                files = row['fastq_ftp'].split(';')\n",
    "            except:\n",
    "                try:\n",
    "                    files = row['download_path'].split(';')\n",
    "                except:\n",
    "                    logger.warning(\"Skipping sample:\" + row['sample_name']\n",
    "                                    + \", run: \" + row['run_accession'] + \"No fastq ftp found.\")\n",
    "            for f in files:\n",
    "                fq_path = output_dir + \"/\" + f.split('/')[-1]\n",
    "                #hd_file_list.append(fq_path)\n",
    "                if DEBUG: logger.info(f)\n",
    "                if type(f) != str:\n",
    "                    logger.warning(\"Skipping sample:\" + row['sample_name']\n",
    "                                   + \", run: \" + row['run_accession'] + \"fastq ftp path is not string.\")\n",
    "                elif path.isfile(fq_path):\n",
    "                    logger.warning(\"Skipping download of \" + fq_path + \" File exists.\")\n",
    "\n",
    "                elif path.isfile(fq_path.replace('.fastq.gz','.R1.fastp.fastq.gz')): #if the quality filtered file exists, also skip\n",
    "                    logger.warning(\"Skipping download of \" + fq_path.replace('.fastq.gz','.R1.fastp.fastq.gz') + \" fastp file exists.\")\n",
    "                elif path.isfile(fq_path.replace('.fastq.gz','.R1.filtered.fastq.gz')) or path.isfile(fq_path.replace('.fastq.gz','.R2.filtered.fastq.gz')): #if the host depleted file exists, also skip\n",
    "                    logger.warning(\"Skipping download of \" + fq_path.replace('.fastq.gz','.R1.filtered.fastq.gz') + \" Host depleted file exists.\")\n",
    "                elif row['sample_name'] in failed_list: #this should only happen if read 1 failes\n",
    "                    logger.warning(\"Skipping download of read 2 for \" + row['sample_name'] + \" . Other read(s) failed to download.\")\n",
    "                else:\n",
    "                #add catch in case there is an issue with the connection\n",
    "                    try:\n",
    "                        urlretrieve(\"ftp://\" +f, fq_path)\n",
    "                        hd_file_list.append(fq_path)\n",
    "                    except:\n",
    "                        logger.warning(\"Issue with urlretrieve for \" + row['sample_name'] + \"Skipping.\")\n",
    "                        failed_list.append(row['sample_name'])\n",
    "        elif mode =='sra':\n",
    "            subprocess.run(['fastq-dump', '-I', '--split-files', '--gzip', '--outdir', output_dir, row['run_accession']])\n",
    "            sra_files = glob.glob(output_dir + row['run_accession'] + '*.gz')\n",
    "            for f in sra_files:\n",
    "                hd_file_list.append(f)\n",
    "\n",
    "        else:\n",
    "            raise Exception(mode + \" is not a valid repository\")\n",
    "\n",
    "        if len(hd_file_list) > 0: #make sure one of the above worked\n",
    "            #run fastqc\n",
    "            if qc:\n",
    "                logger.info('Running fastqc')\n",
    "                run_qc(hd_file_list,output_dir,cpus)\n",
    "\n",
    "            #run quality check if requested\n",
    "            if quality and host_deplete:\n",
    "                logger.warning(\"Quality check run as part of host depletion.\")\n",
    "            elif quality:\n",
    "                run_quality_check(hd_file_list,cpus,output_dir,sequencer)\n",
    "\n",
    "            #enter host depletion checks\n",
    "            valid_hd_prep_types = ['WGS','WGA','WXS','ChIP-Seq','RNA-Seq']\n",
    "\n",
    "            if host_deplete:\n",
    "                if len(hd_file_list) > 2:\n",
    "                    logger.warning(\"More than 2 files in the fastq download path. Skipping host depletion for \"\n",
    "                         + row['sample_name'])\n",
    "                else:\n",
    "                    if row['library_strategy'] in valid_hd_prep_types:\n",
    "                        logger.info(\"Running host depletion on \" + row['sample_name'])\n",
    "                        run_host_depletion(hd_file_list,host_db,cpus,output_dir,hd_method,qc,sequencer)\n",
    "                    elif force_hd:\n",
    "                        logger.warning(\"Forcing host depletion for \" + + \" with library_strategy \" + + \". This may break...\")\n",
    "                        run_host_depletion(hd_file_list,host_db,cpus,output_dir,hd_method,qc,sequencer)\n",
    "                    else:\n",
    "                        logger.warning(\"Skipping host depletion for \" + row['sample_name'] + \" Library Strategy is \"\n",
    "                                     + row['library_strategy'] + \". Valid formats are: \"+ str(valid_hd_prep_types))\n",
    "\n",
    "    #drop samples from dataframe if the download fails\n",
    "    valid_df =download_df[~download_df['sample_name'].isin(failed_list)]\n",
    "\n",
    "    return valid_df\n",
    "\n",
    "### END: EBI/ENA AND NCBI/SRA DATA RETRIEVAL METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START: OUTPUT FILE WRITING METHODS\n",
    "\n",
    "def write_config_file(xml_dict,prefix,mode,id_type='study',xml_proj_dict={}):\n",
    "    \"\"\"Writes the study_config and study_title files needed for Qiita loading\n",
    "\n",
    "    #TODO write description\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xml_dict: dict\n",
    "        distionary of study information for parsing\n",
    "    prefix: string\n",
    "        prefix to assign to validator error log, includes path\n",
    "    mode : string\n",
    "        'ebi' or 'sra'; repo to use for data retrieval\n",
    "    \n",
    "    id_type : string\n",
    "        type of id supplied, either study or project\n",
    "    xml_proj_dict: dict\n",
    "        dictionary of project information for parsing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    empo_type_df : tuple\n",
    "        returns project and study id\n",
    "    \"\"\"\n",
    "\n",
    "    config_string='[required]\\ntimeseries_type_id = 1\\nmetadata_complete = True\\nmixs_compliant = True'\n",
    "\n",
    "    #for now at least, setting the PI to default to Qiita-Help\n",
    "    config_string = config_string + '\\nprincipal_investigator = Qiita-EBI Import, qiita.help@gmail.com, See study details'\n",
    "\n",
    "    config_string = config_string + '\\nreprocess = False'\n",
    "    if mode == 'ebi':\n",
    "        parse_dict = xml_dict['STUDY_SET']['STUDY']\n",
    "    elif mode == 'sra':\n",
    "        parse_dict = xml_dict['STUDY']\n",
    "    else:\n",
    "        logger.warning('Received invalid mode: ' + mode)\n",
    "\n",
    "    title= 'XXEBIXX'\n",
    "    alias= '\\nstudy_alias = XXEBIXX'\n",
    "    abstract ='\\nstudy_abstract = XXEBIXX'\n",
    "    description = '\\nstudy_description = XXEBIXX'\n",
    "    proj_id =parse_dict['IDENTIFIERS']['SECONDARY_ID']\n",
    "    study_id =parse_dict['IDENTIFIERS']['PRIMARY_ID']\n",
    "\n",
    "\n",
    "    if '@alias' in parse_dict.keys():\n",
    "        alias=alias.replace('XXEBIXX',parse_dict['@alias'])\n",
    "    elif '@alias' in xml_proj_dict.keys():\n",
    "        alias=alias.replace('XXEBIXX',_dict['PROJECT_SET']['PROJECT']['@alias'])\n",
    "    else:\n",
    "        logger.warning(\"No alias found, using XXEBIXX for alias.\")\n",
    "\n",
    "    desc_dict={}\n",
    "    if 'DESCRIPTOR' in parse_dict.keys():\n",
    "        desc_dict = parse_dict['DESCRIPTOR']\n",
    "    elif 'IDENTIFIERS' in xml_proj_dict.keys():\n",
    "        desc_dict = xml_proj_dict['IDENTIFIERS']\n",
    "    else:\n",
    "        logger.warning(\"No DESCRIPTOR or IDENTIFIER values found. Using XXEBIXX for values.\")\n",
    "\n",
    "    if len(desc_dict) > 0:\n",
    "        if 'STUDY_ABSTRACT' in desc_dict.keys():\n",
    "            abstract=abstract.replace('XXEBIXX',desc_dict['STUDY_ABSTRACT'])\n",
    "        elif 'ABSTRACT' in desc_dict.keys():\n",
    "            abstract=abstract.replace('XXEBIXX',desc_dict['ABSTRACT'])\n",
    "        else:\n",
    "            logger.warning(\"No abstract found, using XXEBIXX for abstract\")\n",
    "\n",
    "        if 'STUDY_DESCRIPTION' in desc_dict.keys():\n",
    "            abstract=abstract.replace('XXEBIXX',desc_dict['STUDY_DESCRIPTION'])\n",
    "        elif 'DESCRIPTION' in desc_dict.keys():\n",
    "            abstract=abstract.replace('XXEBIXX',desc_dict['DESCRIPTION'])\n",
    "        else:\n",
    "            logger.warning(\"No description found, using XXEBIXX for description\")\n",
    "\n",
    "        if 'STUDY_TITLE' in desc_dict.keys():\n",
    "            title=title.replace('XXEBIXX',desc_dict['STUDY_TITLE'])\n",
    "        elif 'TITLE' in desc_dict.keys():\n",
    "            title=title.replace('XXEBIXX',desc_dict['TITLE'])\n",
    "        else:\n",
    "            logger.warning(\"No title found, using XXEBIXX for title\")\n",
    "\n",
    "    config_string= config_string + alias + description + abstract + '\\nefo_ids = 1\\n[optional]' #To add? + '\\nproject_id = ' + proj_id + '\\nstudy_id = ' + study_id\n",
    "\n",
    "    study_config_file = prefix + '_study_config.txt'\n",
    "    study_title_file = prefix + '_study_title.txt'\n",
    "\n",
    "    # Write out files\n",
    "    c_file = open(study_config_file, \"w\")\n",
    "    c_file.write(config_string)\n",
    "    c_file.close()\n",
    "\n",
    "    t_file = open(study_title_file, \"w\")\n",
    "    t_file.write(title)\n",
    "    t_file.close()\n",
    "\n",
    "    return proj_id, study_id\n",
    "\n",
    "def create_details_file(study_details_df, study_accession,mode='ebi',prefix='',file_suffix=\"_detail\"):\n",
    "    \"\"\"Returns the details of the EBI/SRA study\n",
    "\n",
    "    If the accession ID is valid, generate a .details.txt, and return the\n",
    "    detail file name of this EBI/SRA study. Else return None\n",
    "    \n",
    "    #TODO: consider dropping since no longer used?\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    study_details_df : pd.Dataframe\n",
    "        dataframe of study information to write out\n",
    "    study_accession:string\n",
    "        project or study ID from EBI/ENA or NCBI/SRA\n",
    "    mode : string\n",
    "        'ebi' or 'sra'; repo to use for data retrieval\n",
    "    prefix: string\n",
    "        prefix to assign to validator error log, includes path\n",
    "    file_suffix : string\n",
    "        The suffix for the output study detail file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        study details file name\n",
    "    \"\"\"\n",
    "    if len(prefix)==0:\n",
    "        prefix = study_accession\n",
    "    study_details = prefix + \"_\" + mode + file_suffix + \".txt\"\n",
    "    study_details_df.to_csv(study_details,sep='\\t',header=True,index=False)\n",
    "    return study_details\n",
    "    \n",
    "def write_info_files(final_tuple,max_prep,prefix=''):\n",
    "    \"\"\"Writes out the prep and sample information files\n",
    "\n",
    "    This is the key method that creates the sample_information\n",
    "    and preparation information files needed for Qiita.\n",
    "    \n",
    "    #TODO: complete description\n",
    "    \n",
    "    #TODO: consider dropping since no longer used?\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    final_tuple : tuple\n",
    "        tuple of metadata (0) and prep_info columns (1)\n",
    "    max_prep: int\n",
    "        max number of samples to write into any prep info file\n",
    "    prefix: string\n",
    "        prefix to assign to validator error log, includes path\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    final_df = final_tuple[0]\n",
    "    if max_prep > len(final_df):\n",
    "        max_prep = len(final_df)\n",
    "\n",
    "    prep_info_columns = ['run_prefix','experiment_accession','platform','instrument_model','library_strategy',\n",
    "                         'library_source','library_layout','library_selection','fastq_ftp','ena_checklist',\n",
    "                         'ena_spot_count','ena_base_count','ena_first_public','ena_last_update','instrument_platform',\n",
    "                         'submitted_format','sequencing_method','target_gene','target_subfragment','primer']\n",
    "\n",
    "    #add multiqc_columns\n",
    "    multiqc_cols=['raw_reads','non_human_reads','post_qc_reads','frac_non_human_from_raw','frac_non_human_from_qc']\n",
    "    prep_info_columns = prep_info_columns + multiqc_cols\n",
    "    #add ebi/user supplied prep info columns\n",
    "    ebi_prep_info_cols= final_tuple[1]\n",
    "    for c in ebi_prep_info_cols:\n",
    "        if c not in prep_info_columns:\n",
    "            prep_info_columns.append(c)\n",
    "\n",
    "    amplicon_min_prep_list=['target_gene','target_subfragment','primer']\n",
    "    amplicon_type_preps = ['16S','ITS','18S']\n",
    "    final_df.columns =[scrub_special_chars(col).lower() for col in final_df.columns]\n",
    "\n",
    "    #write sample_info\n",
    "    sample_df=final_df[final_df.columns[~final_df.columns.isin(prep_info_columns)]]\n",
    "\n",
    "    #check for duplicates here. N.B. Need to retain previously to enable download of all runs without faffing around with prep files\n",
    "    sample_df=sample_df.drop_duplicates('sample_name')\n",
    "    sample_df=sample_df.set_index('sample_name').dropna(axis=1,how='all')\n",
    "    sample_df.to_csv(prefix+'_sample_info.tsv',sep='\\t',index=True,index_label='sample_name')\n",
    "\n",
    "    #clean up pre-validation file assuming correct validation\n",
    "    if not DEBUG:\n",
    "        pre_validation_file = prefix + \"_unvalidated_sample_info.part\"\n",
    "        if path.isfile(pre_validation_file):\n",
    "            remove(pre_validation_file)\n",
    "\n",
    "    prep_info_columns = ['sample_name'] + prep_info_columns #add to list for writing out prep files\n",
    "    for prep_file in final_df['prep_file'].unique():\n",
    "\n",
    "        #adding way to check for min essential target gene information where needed\n",
    "        logger.info(prep_file.split('_')[0])\n",
    "        prep_df = final_df[final_df['prep_file']==prep_file]\n",
    "        if prep_file.split('_')[0] in amplicon_type_preps: #check to see if the prep is amplicon-style, specified by list above\n",
    "            for min_prep in amplicon_min_prep_list: #if amplicon-style, enforce presence or null values for minimum prep info information\n",
    "                if min_prep not in prep_df.columns:\n",
    "                    prep_df[min_prep]='XXEBIXX' #will throw warning, but okay with current pandas\n",
    "\n",
    "        #now write out the prep info files\n",
    "        prep_df= prep_df[prep_df.columns[prep_df.columns.isin(prep_info_columns)]].set_index('sample_name')\n",
    "        prep_df=prep_df.dropna(axis=1,how='all')\n",
    "        prep_df_list = [prep_df[i:i+max_prep] for i in range(0,prep_df.shape[0],max_prep)]\n",
    "        prep_count=0\n",
    "        for prep in prep_df_list:\n",
    "            prep.to_csv(prefix+'_prep_info_'+ prep_file + '_part' + str(prep_count) +'.tsv',sep='\\t',index=True,index_label='sample_name')\n",
    "            prep_count += 1\n",
    "            \n",
    "### END: OUTPUT FILE WRITING METHODS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START: FASTQ FILE PROCESSING METHODS\n",
    "\n",
    "def run_quality_check(raw_list,cpus=4,output_dir='./',model='',method='fastp',min_length=45,qc=True,keep=False):\n",
    "    \"\"\"Runs fastp quality filtering and optionally fastqc\n",
    "    \n",
    "    #TODO: complete description, consider dropping fastqc if fastp alternative can be used\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_list: list\n",
    "        list of files to quality filter\n",
    "    cpus : int\n",
    "        number of threads to use for fastp and fastq\n",
    "    output_dir : string\n",
    "        the path to save the output files\n",
    "    model : string\n",
    "       the model of instrument used for sequencing\n",
    "    method : string\n",
    "        method for qc, only fastp supported currently\n",
    "    min_length : int\n",
    "        minimum sequencing length for quality filtering; set to 45 to permit RNA-Seq data\n",
    "    qc : boolean\n",
    "        whether to run fastqc on output\n",
    "    keep : boolean\n",
    "        whether to keep the raw files after filtering\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    return_list : list\n",
    "        list of quality-filtered files\n",
    "    \n",
    "    \"\"\"\n",
    "    valid_methods = ['fastp']\n",
    "    polyG_model_list = ['Illumina MiniSeq', 'Illumina NovaSeq 6000', 'NextSeq 500', 'NextSeq 550']\n",
    "\n",
    "    read1_fastq = raw_list[0]\n",
    "\n",
    "    if not method in valid_methods:\n",
    "        logger.warning(\"Method \" + method + \" not supported. Available quality check methods: \"\n",
    "                       + valid_methods + \". Returning raw files: \" + raw_list)\n",
    "        return raw_list\n",
    "\n",
    "    elif method == 'fastp':\n",
    "        qc_fastq1 = read1_fastq.replace('.fastq.gz','.R1.fastp.fastq.gz')\n",
    "        qc_fastq2 = '' #dummy file to help with cleanup\n",
    "        return_list=[qc_fastq1]\n",
    "\n",
    "        #set fastp parameters\n",
    "        fastp_args=['fastp','-l',str(min_length),'-i', read1_fastq,'-o',qc_fastq1]\n",
    "        #to determine %microbial reads, need to write out and fastqc files. will assume this is default when host depleting\n",
    "\n",
    "        if len(raw_list)==2:\n",
    "            read2_fastq = raw_list[1]\n",
    "            qc_fastq2 = read2_fastq.replace('.fastq.gz','.R2.fastp.fastq.gz')\n",
    "            fastp_args = fastp_args + ['-I',read2_fastq,'-O',qc_fastq2]\n",
    "            return_list.append(qc_fastq2)\n",
    "\n",
    "        #add final flags for fastp after determining input file count\n",
    "        #fastp_html =read1_fastq.replace('_1','')+'_fastp.html' #TODO evaluate if fastp can replace fastqc?\n",
    "        fastp_args = fastp_args + ['-w',str(cpus)] #,'-h',fastp_html]\n",
    "        if model in polyG_model_list:\n",
    "            fastp_args + fastp_args + ['-g','--poly_g_min_len'] #polyG filtering, 10 is default\n",
    "\n",
    "        #see if file already exists\n",
    "        if path.isfile(qc_fastq1): #process should make both, so just check\n",
    "            logger.warning(\"Output fastp file already exists for \" + read1_fastq + \" Skipping.\")\n",
    "\n",
    "        else:\n",
    "            fastp_ps = subprocess.Popen(fastp_args)\n",
    "            fastp_ps.wait()\n",
    "            logger.info(\"past fastp\")\n",
    "\n",
    "        #clean up raw file if qc done\n",
    "        if path.isfile(qc_fastq1):\n",
    "            if qc: #very likely\n",
    "                result=run_qc([qc_fastq1],output_dir,cpus)\n",
    "            if not keep:\n",
    "                subprocess.run(['rm', read1_fastq])\n",
    "        if path.isfile(qc_fastq2):\n",
    "            if qc: #very likely\n",
    "                result=run_qc([qc_fastq2],output_dir,cpus)\n",
    "            if not keep:\n",
    "                subprocess.run(['rm', read2_fastq])\n",
    "\n",
    "        if DEBUG: logger.info(\"Return list is: \" + str(return_list))\n",
    "        return return_list\n",
    "\n",
    "def run_host_depletion(fastq_file_list,filter_db='',cpus=4,output_dir='./',method='minimap2',qc=True,model='',keep=False,min_length=45):\n",
    "    \"\"\"Quality filter and host deplete list of files\n",
    "    \n",
    "    #TODO: complete description\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fastq_file_list: list\n",
    "        list of raw files to quality filter and host deplete\n",
    "    filter_db : string\n",
    "        path to the databased to be used for filtering\n",
    "    cpus : int\n",
    "        number of threads to use for fastp and fastq\n",
    "    output_dir : string\n",
    "        the path to save the output files\n",
    "    method : string\n",
    "        method for host depletion\n",
    "    model : string\n",
    "       the model of instrument used for sequencing\n",
    "    qc : boolean\n",
    "        whether to run fastqc on output\n",
    "    keep : boolean\n",
    "        whether to keep the raw files after quality filtering, and fastp files after host filtering    \n",
    "    min_length : int\n",
    "        minimum sequencing length for quality filtering; set to 45 to permit RNA-Seq data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    db_dict = {'bowtie2':'/databases/bowtie/Human_phiX174/Human_phix174',\n",
    "                'minimap2':'/databases/minimap2/human-phix-db.mmi'\n",
    "              }\n",
    "    if filter_db == '':\n",
    "        filter_db = db_dict[method]\n",
    "\n",
    "    read1_fastq=str(fastq_file_list[0])\n",
    "    filtered_fastq_1=read1_fastq.replace('.fastq.gz','.R1.filtered.fastq')\n",
    "    output_fastq1=filtered_fastq_1 + '.gz'\n",
    "\n",
    "    #new helper list for qc checks\n",
    "    to_qc=[read1_fastq]\n",
    "\n",
    "    if len(fastq_file_list) == 2:\n",
    "        read2_fastq=str(fastq_file_list[1])\n",
    "        filtered_fastq_2=read2_fastq.replace('.fastq.gz','.R2.filtered.fastq')\n",
    "        output_fastq2 =filtered_fastq_2 + '.gz'\n",
    "        to_qc.append(read2_fastq)\n",
    "    else:\n",
    "        read2_fastq = '' #to reduce code complexity for checking for paired vs unpaired, dummy file string here\n",
    "        output_fastq2 = '' #dummy file to help with cleanup \n",
    "\n",
    "\n",
    "    #run some checks to help wtih resumed runs\n",
    "    skip = False\n",
    "    skip = path.isfile(output_fastq1)\n",
    "    if output_fastq2 != '':\n",
    "        skip = path.isfile(output_fastq2)\n",
    "\n",
    "    if skip:\n",
    "        logger.warning(\"All filtered files found for \" + read1_fastq.replace('.fastq.gz','').split('/')[-1] + \". Skipping host depletion.\")\n",
    "\n",
    "    elif method == 'bowtie2':\n",
    "        if DEBUG: logger.info(\"Starting bowtie2 depletion\")\n",
    "        bowtie2_args=['bowtie2', '-p', str(cpus), '-x', filter_db]\n",
    "        stv_args_1 =['samtools', 'view','-F', '256']\n",
    "        sts_args =['samtools', 'sort', '-@', str(cpus),'-n']\n",
    "        stv_args_2 = ['samtools', 'view','-bS']\n",
    "        btb_args = ['bedtools', 'bamtofastq', '-i', '-', '-fq', filtered_fastq_1]\n",
    "\n",
    "        if len(fastq_file_list) == 2:\n",
    "            bowtie2_args=bowtie2_args + ['-1',read1_fastq,'-2',read2_fastq]\n",
    "            stv_args_1 = stv_args_1 + ['-f','12']\n",
    "            btb_args = btb_args + ['-fq2',filtered_fastq_2]\n",
    "        else:\n",
    "            bowtie2_args=bowtie2_args + ['-U',read1_fastq]\n",
    "            stv_args_1 = stv_args_1 + ['-f','4']\n",
    "\n",
    "        bowtie2_args.append('--fast-local')\n",
    "\n",
    "        #now run bowtie2 commands in chain\n",
    "        bt2_ps = subprocess.Popen(bowtie2_args, stdout=subprocess.PIPE)\n",
    "        stv_ps1 = subprocess.Popen(stv_args_1,stdin=bt2_ps.stdout, stdout=subprocess.PIPE)\n",
    "        sts_ps = subprocess.Popen(sts_args,stdin=stv_ps1.stdout, stdout=subprocess.PIPE)\n",
    "        stv_ps2 = subprocess.Popen(stv_args_2,stdin=sts_ps.stdout, stdout=subprocess.PIPE)\n",
    "        btb_ps = subprocess.Popen(btb_args,stdin=stv_ps2.stdout, stdout=subprocess.PIPE)\n",
    "        btb_ps.wait()\n",
    "\n",
    "        if path.isfile(filtered_fastq_1):\n",
    "            subprocess.run(['gzip', filtered_fastq_1])\n",
    "            run_qc([output_fastq1],output_dir,cpus)\n",
    "            if not keep:\n",
    "                subprocess.run(['rm', read1_fastq])\n",
    "        if path.isfile(filtered_fastq_2):\n",
    "            subprocess.run(['gzip', filtered_fastq_2])\n",
    "            run_qc([output_fastq2],output_dir,cpus)\n",
    "            if not keep:\n",
    "                subprocess.run(['rm', read2_fastq])\n",
    "\n",
    "    elif method == 'minimap2':\n",
    "\n",
    "        #separating out filtering for flexibility and % microbial read determination\n",
    "        qced_fastq=run_quality_check(to_qc,cpus,output_dir,model)\n",
    "        if DEBUG: logger.info(\"Starting minimap2 depletion.\")\n",
    "        minimap2_args = ['minimap2','-ax', 'sr', '-t', str(cpus), filter_db,'-a'] + qced_fastq\n",
    "        stf_args=['samtools','fastq', '-@', str(cpus),'-F', '256', '-']\n",
    "\n",
    "        if len(fastq_file_list)==2:\n",
    "            stf_args= stf_args + ['-f', '12','-1',output_fastq1,'-2',output_fastq2]\n",
    "        else: #different samtools parameter for unpaired\n",
    "            stf_args= stf_args + ['-f', '4','-0',output_fastq1]\n",
    "\n",
    "        minimap2_ps = subprocess.Popen(minimap2_args, stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "        stf_ps = subprocess.Popen(stf_args,stdin=minimap2_ps.stdout, stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "        stf_ps.wait()\n",
    "\n",
    "        #remove source files if not needed\n",
    "        if path.isfile(output_fastq1):\n",
    "            run_qc([output_fastq1],output_dir,cpus)\n",
    "            if not keep:\n",
    "                #subprocess.run(['rm', read1_fastq]) #moved to post quality\n",
    "                subprocess.run(['rm', qced_fastq[0]])\n",
    "\n",
    "        if path.isfile(output_fastq2):\n",
    "            run_qc([output_fastq2],output_dir,cpus)\n",
    "            if not keep:\n",
    "                #subprocess.run(['rm', read2_fastq]) #moved to post-quality\n",
    "                subprocess.run(['rm', qced_fastq[1]])\n",
    "    else:\n",
    "        logger.warning(\"Selected depletion method '\" + method + \"' not currently supported. Please select either bowtie2 or minimap2.\")\n",
    "\n",
    "def run_qc(input_list,output_dir='./',cpus=4,multiqc=False):\n",
    "    \"\"\"Run fastqc and/or multiqc list of files\n",
    "    \n",
    "    #TODO: complete description\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_list: list\n",
    "        list of files to qc\n",
    "    output_dir : string\n",
    "        the path to save the output files\n",
    "    cpus : int\n",
    "        number of threads to use for fastp and fastq\n",
    "\n",
    "    multiqc : boolean\n",
    "        whether to run multiqc on the output directory\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    'qc_done': string\n",
    "        indicator that qc completed    \n",
    "    \n",
    "    \"\"\"\n",
    "    #check to see if fastqc has been run on the files in output, and if not, run it\n",
    "    output_dir = output_dir + '/fastqc/'\n",
    "    if not path.exists(output_dir):\n",
    "        makedirs(output_dir)\n",
    "    if not len(input_list) == 0: #bypass fastqc run by passing empty list\n",
    "        fastqc_args = ['fastqc']\n",
    "\n",
    "        for f in input_list:\n",
    "            fastqc_name = output_dir+f.split('/')[-1].replace('.fastq.gz','_fastqc.html')\n",
    "\n",
    "            if not path.isfile(fastqc_name):\n",
    "                fastqc_args = fastqc_args + [f]\n",
    "\n",
    "        #add a check to ensure there is something to run\n",
    "        if len(fastqc_args) != 1:\n",
    "            fastqc_args = fastqc_args + ['-t',str(cpus),'-o',output_dir]\n",
    "            if not DEBUG:\n",
    "                fastqc_args = fastqc_args + ['-q']\n",
    "\n",
    "            fastqc_ps = subprocess.Popen(fastqc_args)\n",
    "            fastqc_ps.wait()\n",
    "\n",
    "    if multiqc:\n",
    "        multiqc_args = ['multiqc',output_dir,'-o',output_dir,'-s','-f']\n",
    "        subprocess.run(multiqc_args)\n",
    "\n",
    "    return 'qc_done'\n",
    "\n",
    "def add_qc_data(partial_df,output_dir='./'):\n",
    "    \"\"\"Automatically augment prep information with read data and % microbial reads\n",
    "    \n",
    "    #TODO: complete description\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    partial_df: pd.DataFrame\n",
    "        dataframe of combined sample and prep information for augmenting\n",
    "    \n",
    "    output_dir : string\n",
    "        the path to save the output files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    partial_df: pd.DataFrame\n",
    "        augmented dataframe with read information\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #read in multiqc data\n",
    "    multiqc_data_file = output_dir+'/multiqc_data/multiqc_fastqc.txt'\n",
    "    try:\n",
    "        mqc = pd.read_csv(multiqc_data_file,sep='\\t')\n",
    "    except:\n",
    "        logger.warning(\"Multiqc data file \" + multiqc_data_file + \" missing. Please check path or re-run with -qc flag.\")\n",
    "\n",
    "    #add catch to skip with warning if the above fails\n",
    "    if len(mqc) > 0:\n",
    "        raw_files = mqc[~mqc['Sample'].str.contains('.fastp')] #drops .fastp\n",
    "        raw_files = raw_files[~raw_files['Sample'].str.contains('.filtered')] #drops filtered\n",
    "        fastp_files = mqc[mqc['Sample'].str.contains('.fastp')] #drops .fastp and .filtered\n",
    "        filt_files = mqc[mqc['Sample'].str.contains('.filtered')] #drops .fastp and .filtered\n",
    "\n",
    "        #to date read 1 length = read 2 length, so assume this for now. can update to sum with groupby and compare later\n",
    "        raw_files['run_prefix'] = raw_files['Sample'].apply(lambda x: x.split('.')[0].split('_')[0]) #accounts for unpaired and paired files\n",
    "        raw_files = raw_files.drop_duplicates(subset='run_prefix',keep = 'first')\n",
    "        raw_files = raw_files.rename({'Total Sequences':'raw_reads'},axis=1)\n",
    "        raw_files = raw_files[['run_prefix','raw_reads']]\n",
    "\n",
    "        fastp_files['run_prefix'] = fastp_files['Sample'].apply(lambda x: x.split('.')[0].split('_')[0]) #accounts for unpaired and paired files\n",
    "        fastp_files = fastp_files.rename({'Total Sequences':'post_qc_reads'},axis=1)\n",
    "        fastp_files = fastp_files.drop_duplicates(subset='run_prefix',keep = 'first')\n",
    "        fastp_files = fastp_files[['run_prefix','post_qc_reads']]\n",
    "\n",
    "        filt_files['run_prefix'] = filt_files['Sample'].apply(lambda x: x.split('.')[0].split('_')[0]) #accounts for unpaired and paired files\n",
    "        filt_files = filt_files.rename({'Total Sequences':'non_human_reads'},axis=1)\n",
    "        filt_files = filt_files.drop_duplicates(subset='run_prefix',keep = 'first')\n",
    "        filt_files = filt_files[['run_prefix','non_human_reads']]\n",
    "\n",
    "        #now merge everything\n",
    "        merge_raw=partial_df.merge(raw_files,how='left',left_on='run_prefix',right_on='run_prefix')\n",
    "        merge_fastp=merge_raw.merge(fastp_files,how='left',left_on='run_prefix',right_on='run_prefix')\n",
    "        merge_filt=merge_fastp.merge(filt_files,how='left',left_on='run_prefix',right_on='run_prefix')\n",
    "\n",
    "        #last calculate fraction\n",
    "        merge_filt['frac_non_human_from_raw']=merge_filt['non_human_reads']/merge_filt['raw_reads']\n",
    "        merge_filt['frac_non_human_from_qc']=merge_filt['non_human_reads']/merge_filt['post_qc_reads']\n",
    "\n",
    "        return merge_filt\n",
    "    else:\n",
    "        logger.warning(\"No multiqc file found. Check output directory and paths and try again. Returning unchanged dataframe.\")\n",
    "        return partial_df\n",
    "\n",
    "\n",
    "### END: FASTQ FILE PROCESSING METHODS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###START: MAIN FUNCTION\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #set up logging\n",
    "    handler = logging.StreamHandler()\n",
    "    fmt_str = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    "    handler.setFormatter(logging.Formatter(fmt_str))\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "    #set up parameters options and defaults\n",
    "    parser = ArgumentParser(description='Please note that the following ' +\n",
    "                            'packages have to be installed for running this ' +\n",
    "                            'script: 1)lxml 2)pandas 3)glob 4)csv 5)sys ' +\n",
    "                            '6)urllib 7)argparse 8)requests 9)xmltodict ' +\n",
    "                            '10)subprocess 11)bioconda 12)sra-tools 13)os ' +\n",
    "                            '14)entrez-direct 15)pyyaml')\n",
    "    parser.add_argument(\"-project\",\"--project\", nargs='*',\n",
    "                        help=\"EBI/ENA project or study accession(s) \" +\n",
    "                        \"to retrieve\")\n",
    "    parser.add_argument(\"-o\",\"--output\", default='./',\n",
    "                        help='directory for output files. Default is working directory.')\n",
    "    parser.add_argument(\"-mode\", \"--mode\", default='ebi',\n",
    "                        help=\"sra accession \" +\n",
    "                        \"repository to be queried.\", choices=['ebi','sra'])\n",
    "    parser.add_argument(\"-prefix\", \"--prefix\", nargs='*',\n",
    "                        help=\"prefix(es) to prepend to output info files\")\n",
    "    parser.add_argument(\"-src\",\"--sources\",nargs='*',choices=['GENOMIC','GENOMIC SINGLE CELL','TRANSCRIPTOMIC',\n",
    "                                                              'TRANSCRIPTOMIC SINGLE CELL','METAGENOMIC',\n",
    "                                                              'METATRANSCRIPTOMIC','SYNTHETIC','VIRAL RNA','OTHER'],\n",
    "                        help=\"list of one or more sources for restricting sample selection.\")\n",
    "    parser.add_argument(\"-strat\",\"--strategies\",nargs='*',choices=['POOLCLONE','CLONE','CLONEEND','WGS','WGA',\n",
    "                                                       'WCS','WXS','AMPLICON','ChIP-Seq','RNA-Seq',\n",
    "                                                       'MRE-Seq','MeDIP-Seq','MBD-Seq','MNase-Seq',\n",
    "                                                       'DNase-Hypersensitivity','Bisulfite-Seq','EST',\n",
    "                                                       'FL-cDNA','miRNA-Seq','ncRNA-Seq','FINISHING',\n",
    "                                                       'TS','Tn-Seq','VALIDATION','FAIRE-seq','SELEX',\n",
    "                                                       'RIP-Seq','ChIA-PET','RAD-Seq','Other'],\n",
    "                        help=\"list of one or more libary strategies to restrict sample selection.\")\n",
    "    parser.add_argument(\"-plat\", \"--platforms\", nargs='*', choices=['LS454','Illumina','Ion Torrent','PacBio_SMRT','OXFORD_NANOPORE'],\n",
    "                        help=\"List of one or more platforms to restrict sample selection.\")\n",
    "    parser.add_argument(\"-name\",\"--scientific_names\",nargs='*',\n",
    "                        help=\"List of scientific_names to restrict for selection.\")\n",
    "    parser.add_argument(\"-yaml\",\"--validators\",nargs='*',\n",
    "                        help=\"one or more yaml files in QIIMP format for validation.\")\n",
    "    parser.add_argument(\"-yaml-dir\",\"--yaml_dir\", default ='./',\n",
    "                        help=\"One or more yaml files in QIIMP format for validation. Loads yml files in ./ by default.\")\n",
    "    parser.add_argument(\"-no-seqs\", \"--no_seqs\", default=False,action='store_true',\n",
    "                        help=\"Omit download of fastq files.\")\n",
    "    parser.add_argument(\"-v\", \"--verbose\", default=False, action='store_true',\n",
    "                        help=\"Output additional messages.\")\n",
    "    parser.add_argument(\"-log\", \"--log\",default='./output.log',\n",
    "                        help=\"filename for logger. Defaults to [output_dir]/[ProjectID]_output.log\")\n",
    "    parser.add_argument(\"-prep-max\",\"--prep_max\",type=int, default=10000,\n",
    "                        help=\"Max number of samples per prep info file.\")\n",
    "    parser.add_argument(\"-f\",\"--force_yaml\",default=False, action='store_true',\n",
    "                        help=\"Advanced: force use of specified yaml for validation.\")\n",
    "    parser.add_argument(\"-hd\",\"--host_deplete\",default=False, action='store_true',\n",
    "                        help=\"Advanced: host deplete using bowtie2. Uses human_PhiX db on barnacle by default.\")\n",
    "    parser.add_argument(\"-db\",\"--host_db\",\n",
    "                        help=\"Advanced: specify the path to the host database for depletion.\")\n",
    "    parser.add_argument(\"-p\",\"--cpus\",type=int, default=4,\n",
    "                        help=\"Number of processors to use during host depletion. Default is 4.\")\n",
    "    parser.add_argument(\"-fhd\",\"--force_host_depletion\",default=False, action='store_true',\n",
    "                        help=\"Advanced: force host depletion for non-supported libary strategies. May break.\")\n",
    "    parser.add_argument(\"-map\",\"--empo_mapping\",\n",
    "                        help=\"Advanced: .tsv with sample_type to EMPO mappings to use for normalization. \" \n",
    "                                                    + \"Should contain the following columns: \" \n",
    "                                                    + \" [sample_type, simple_sample_type, empo_1, empo_2,empo_3]\")\n",
    "    parser.add_argument(\"-method\",\"--depletion_method\",default='bowtie2',\n",
    "                        help=\"Advanced: set host depletion method. bowtie2 by default. TODO implement minimap2 as second option.\")\n",
    "    parser.add_argument(\"-qc\",\"--run_qc\",default=False,action='store_true',\n",
    "                        help=\"Advanced: run fastqc and generate multiqc report on downloaded and host-depleted files.\")\n",
    "    parser.add_argument(\"-sip\",\"--max_samples\",type=int,\n",
    "                        help=\"Advanced: Max number of samples to grab from the study.\")\n",
    "    parser.add_argument(\"-rand\",\"--random\",default=False,action='store_true',\n",
    "                        help=\"Advanced: when sampling, randomly select subset for processing. N.B. must supply a number\"\n",
    "                             + \" with -sip (or --max_samples).\")\n",
    "    parser.add_argument(\"-qual\",\"--quality_filter\",default=False,action='store_true',\n",
    "                        help=\"Advanced: run quality filtering of raw data. Automatically enabled when host depleting.\")\n",
    "\n",
    "    # parse the flags and initialize output file names\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.project is None:\n",
    "        logger.warning(\"\"\"\n",
    "                python EBI_SRA_Downloader.py -project [accession] [accession ... N]\n",
    "                    Generate the study info, study detail, prep, and  sample\n",
    "                    files for the entered EBI accession, and download the\n",
    "                    FASTQ files.\n",
    "                Optional flags:\n",
    "                    -output [directory where files will be saved]\n",
    "                    -mode [specifies which repository to use]                    \n",
    "                    -prefix [list of prefixes for sample and prep info files]\n",
    "                    --strategy [list of one or more library strategies to select]\n",
    "                    --sources [list of one or more library sources to select]\n",
    "                    --platforms [list of one or more sequencing platforms to select]\n",
    "                    --scientific_names [list of one or more scientific names to select]\n",
    "                    --validators [list of one or more yaml files to use in validating]\n",
    "                    --no_seqs [skip downloading files]\n",
    "                    --prep_max [Max number of samples per prep info file: https://qiita.ucsd.edu/static/doc/html/faq.html? \n",
    "                      highlight=size#how-should-i-split-my-samples-within-preparations]\n",
    "                    --verbose\n",
    "               \"\"\")\n",
    "        sys.exit(2)\n",
    "    else:\n",
    "        #settings\n",
    "        mode=args.mode \n",
    "        DEBUG = args.verbose \n",
    "        omit_seqs = args.no_seqs \n",
    "        force = args.force_yaml\n",
    "        max_prep =args.prep_max\n",
    "        host_deplete = args.host_deplete\n",
    "        proc= args.cpus\n",
    "        force_hd = args.force_host_depletion\n",
    "        hd_method=args.depletion_method\n",
    "        qc = args.run_qc\n",
    "        quality_filter=args.quality_filter\n",
    "        random_sample = args.random\n",
    "\n",
    "        if args.max_samples is not None:\n",
    "            max_samples = args.max_samples\n",
    "            subset = True\n",
    "        else:\n",
    "            subset = False\n",
    "\n",
    "        if random_sample and subset:\n",
    "            raise Exception(\"Must supply max_samples when requesting a random sample selection.\")\n",
    "\n",
    "        db_dict = {'bowtie2':'/databases/bowtie/Human_phiX174/Human_phix174',\n",
    "                   'minimap2':'/databases/minimap2/human-phix-db.mmi'\n",
    "                   }\n",
    "\n",
    "        if args.host_db is not None:\n",
    "            host_db = args.host_db\n",
    "        else:\n",
    "            try:\n",
    "                host_db=db_dict[hd_method]\n",
    "            except:\n",
    "                raise Exception(\"No database found for host depletion method \" + hd_method + \" Please check spelling and try again.\")\n",
    "\n",
    "        if args.log is not None:\n",
    "            fh=logging.FileHandler(args.log)\n",
    "            logger.addHandler(fh)\n",
    "\n",
    "        if DEBUG: logger.setLevel(logging.INFO)\n",
    "\n",
    "        # Output directory\n",
    "        output = args.output\n",
    "        if not path.exists(output):\n",
    "            makedirs(output)\n",
    "        if list(output)[-1] != '/':\n",
    "            output = output + '/'\n",
    "\n",
    "        #set up validators\n",
    "        yaml_validator_dict = {}\n",
    "        yaml_list =  []\n",
    "\n",
    "        if args.validators is not None:\n",
    "            for y in args.validators:\n",
    "                yaml_list.append(y)\n",
    "        else:\n",
    "            for file in os.listdir(args.yaml_dir):\n",
    "                if file.endswith(\".yml\") or file.endswith(\".yaml\"):\n",
    "                    yaml_list.append(os.path.join(args.yaml_dir, file))\n",
    "\n",
    "        #since we're providing a way to override parsing, check assumption that a single validator is being passed\n",
    "        if force:\n",
    "            if len(yaml_list) > 1:\n",
    "                raise Exception(\"Error: more than one yaml supplied with 'force' mode. Please supply only one yaml file.\" +\\\n",
    "                                \"Note .yml and .yaml files in working directory (or specified by --yaml_dir) are loaded if \" +\\\n",
    "                                \" no -yaml flag is supplied. Loaded: \" + str(yaml_list))\n",
    "            elif len(yaml_list) == 0:\n",
    "                raise Exception(\"Error: no yaml supplied with 'force' mode. Please supply only one yaml file.\")\n",
    "            else:\n",
    "                yaml_validator_dict = yaml_list\n",
    "        else:\n",
    "            yaml_validator_dict = set_yaml_validators(yaml_list)\n",
    "\n",
    "        if args.empo_mapping is not None:\n",
    "            empo_mapping_dict = set_empo_normalizers(args.empo_mapping)\n",
    "        else:\n",
    "            empo_mapping_dict = {}\n",
    "\n",
    "        platforms=[]\n",
    "        if args.platforms is not None:\n",
    "            for p in args.platforms:\n",
    "                platforms.append(p.lower())\n",
    "\n",
    "        strategies =[]\n",
    "        if args.strategies is not None:\n",
    "            for s in args.strategies:\n",
    "                strategies.append(s.lower())\n",
    "\n",
    "        names=[]\n",
    "        if args.scientific_names is not None:\n",
    "            for n in args.scientific_names:\n",
    "                names.append(n.lower())\n",
    "\n",
    "        sources=[]\n",
    "        if args.sources is not None:\n",
    "            for c in args.sources:\n",
    "                sources.append(c.lower())\n",
    "\n",
    "        # Retreive study information\n",
    "        p_count=0\n",
    "        for p in args.project:\n",
    "            if args.prefix is not None:\n",
    "                if len(args.prefix) == 1:\n",
    "                    file_prefix = output + args.prefix[0] + '_' + p\n",
    "                elif len(args.prefix) == len(args.project):\n",
    "                    file_prefix = output + args.prefix[p_count] + '_' + p\n",
    "                else:\n",
    "                    raise Exception(\"Number of prefixes does not match number of projects. Set a single prefix or matched prefixes.\")\n",
    "            else:\n",
    "                file_prefix = output + p\n",
    "            p_count +=1    \n",
    "\n",
    "            study = get_study_details(p,mode,file_prefix)\n",
    "\n",
    "            if subset:\n",
    "                study = sub_sample_study(study,max_samples,random_sample)\n",
    "\n",
    "            #get and tidy sample and prep metadata\n",
    "            md_tuple=get_sample_info(study,mode,platforms,strategies,yaml_validator_dict,file_prefix,names,sources,empo_mapping_dict)\n",
    "\n",
    "            #write out sample and prep info files for reference while downloading\n",
    "            write_info_files(md_tuple,max_prep,file_prefix)\n",
    "\n",
    "            if not omit_seqs:\n",
    "                valid_samples = fetch_sequencing_data(md_tuple[0],output,mode,host_deplete,host_db,proc,hd_method,qc,quality_filter)\n",
    "                md_tuple = valid_samples, md_tuple[1]\n",
    "                \n",
    "\n",
    "            #run multiqc if requested\n",
    "            if qc:\n",
    "                multiqc_list = glob.glob(output + '/*_fastqc.html')\n",
    "                run_qc(multiqc_list,output,proc,True)\n",
    "                if host_deplete:\n",
    "                    qc_df = add_qc_data(md_tuple[0],output+'/fastqc')\n",
    "                    md_tuple = qc_df, md_tuple[1]\n",
    "\n",
    "                    #re-write out sample and prep info files if qc+host_deplete performed since % non-human reads will be added\n",
    "                    write_info_files(md_tuple,max_prep,file_prefix)\n",
    "\n",
    "            #kludge to handle output log for now\n",
    "            if path.isfile('./output.log'):\n",
    "                shutil.move('./output.log',file_prefix + '_output.log')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
