{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries used\n",
    "import csv\n",
    "import sys\n",
    "import glob\n",
    "import logging\n",
    "import requests\n",
    "import subprocess\n",
    "from xmltodict import parse\n",
    "from lxml import etree\n",
    "import os\n",
    "from os import path, makedirs\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "from pandas import read_csv, DataFrame\n",
    "import re\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set constants\n",
    "handler = logging.StreamHandler()\n",
    "fmt_str = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    "handler.setFormatter(logging.Formatter(fmt_str))\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "#need to work this out to write log\n",
    "#fh = logging.FileHandler('test.log')\n",
    "#logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_caps(cap_string):\n",
    "    temp_list = []\n",
    "    temp_list = re.findall('[A-Z][^A-Z]*', cap_string)\n",
    "    if len(temp_list) > 0:\n",
    "        return \"_\".join(temp_list)\n",
    "    else:\n",
    "        return cap_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_special_chars(input_string,custom_dict={}):\n",
    "    replace_dict ={\"__\":\"_\",\n",
    "                   \" \":\"_\",\n",
    "                   \"-\": \"_\",\n",
    "                   \"(\": \"_leftparen_\",\n",
    "                   \")\": \"_rightparen_\",\n",
    "                   \"/\": \"_per_\",\n",
    "                   \"-\":\"_\",\n",
    "                   \"|\":\"_bar_\",\n",
    "                   \"~\":\"_tilde_\",\n",
    "                   \"`\":\"_\",\n",
    "                   \"@\":\"_at_\",\n",
    "                   \"#\":\"_number_\",\n",
    "                   \"$\":\"dollar\",\n",
    "                   \"%\":\"_perc_\",\n",
    "                   \"^\":\"_carrot_\",\n",
    "                   \"&\":\"_and_\",\n",
    "                   \"*\":\"_star_\",\n",
    "                   \"+\":\"_plus\",\n",
    "                   \"=\":\"_equals\",\n",
    "                   \"\\\\\":\"_per_\",\n",
    "                   \"{\":\"_leftbracket_\",\n",
    "                   \"}\":\"_rightbracket_\",\n",
    "                   \"[\":\"_leftbracket_\",\n",
    "                   \"]\":\"_rightbracket_\",\n",
    "                   \"?\":\"_question\",\n",
    "                   \"<\":\"_less_\",\n",
    "                   \">\":\"_more_\",\n",
    "                   \",\":\"_\",\n",
    "                   \".\":\"_\",}\n",
    "    for k in replace_dict.keys():\n",
    "        input_string=input_string.replace(k,replace_dict[k])\n",
    "    for k in custom_dict.keys():\n",
    "        input_string=input_string.replace(k,custom_dict[k])\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qiimp_parser(filename):\n",
    "    ext=Path(filename).suffix\n",
    "    parsed_yml={}\n",
    "    if ext == '.xlsx':\n",
    "        try:\n",
    "            temp_yml=pd.read_excel(filename,sheet_name='metadata_schema',header=None) #assume QIIMP-style excel\n",
    "            parsed_yml = yaml.load(temp_yml.at[0,0])\n",
    "        except:\n",
    "            logger.warning(\"Invalid .xlsx file. Please ensure file is from QIIMP or contains a compliant yaml \" + \n",
    "                          \"in cell A1 of the sheet labelled 'metadata_schema'.\")\n",
    "    elif ext == '.yml' or ext == '.yaml':\n",
    "        with open(filename) as file:\n",
    "            # The FullLoader parameter handles the conversion from YAML\n",
    "            # scalar values to Python the dictionary format\n",
    "            parsed_yml=yaml.load(file, Loader=yaml.FullLoader)                   \n",
    "    else:\n",
    "        logger.warning(\"Invalid file extension for yaml parsing: \" + str(ext))\n",
    "    \n",
    "    if len(parsed_yml) == 0:\n",
    "        logger.warning(\"The file \" + filename +\" contains no yaml data. Please check contents and try again.\")\n",
    "    \n",
    "    return parsed_yml             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_yaml_validators(validators,field='scientific_name'):\n",
    "    yaml_dict={}\n",
    "    for v in validators:\n",
    "        new_yaml={}\n",
    "        test=qiimp_parser(v)\n",
    "        #print(test)\n",
    "        print(test.keys())\n",
    "        try:\n",
    "            new_yaml = qiimp_parser(v)\n",
    "        except:\n",
    "            logger.warning(\"Could not open yaml file \" + v + \" Please ensure the file exists and is a valid yaml file.\")\n",
    "        \n",
    "        if field not in new_yaml.keys():\n",
    "            logger.warning(\"Invalid validator yaml. Please ensure a default value is provided for '\" + field +\n",
    "                      \"'. Available keys in file: \" + str(new_yaml.keys()))\n",
    "        elif 'default' not in new_yaml[field].keys():\n",
    "            logger.warning(\"Invalid validator yaml. Please ensure a default value is provided for '\" + field +\n",
    "                      \"'. Available keys in : \" + field + str(new_yaml[field].keys()))\n",
    "        else:\n",
    "            yaml_dict[new_yaml[field]['default']] = v\n",
    "            \n",
    "    return yaml_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these functions create the study_details files for ebi and sra respectively\n",
    "\n",
    "def get_study_details(study_accession,mode='ebi'):\n",
    "    \n",
    "    if mode == 'ebi':        \n",
    "        host = \"http://www.ebi.ac.uk/ena/data/warehouse/filereport?accession=\"\n",
    "        read_type = \"&result=read_run&\"\n",
    "        fields = \"library_name,secondary_sample_accession,run_accession,\" + \\\n",
    "                 \"experiment_accession,fastq_ftp,library_source,\" + \\\n",
    "                 \"instrument_platform,submitted_format,library_strategy,\" +\\\n",
    "                 \"library_layout,tax_id,scientific_name,instrument_model,\" + \\\n",
    "                \"library_selection,center_name,experiment_title,\" +\\\n",
    "                \"study_title,study_alias,experiment_alias,sample_alias,sample_title\"\n",
    "        url = ''.join([host, study_accession, read_type, \"fields=\", fields])\n",
    "        if DEBUG: logger.info(url)\n",
    "        try:\n",
    "            study_df = pd.read_csv(url,sep='\\t')\n",
    "            study_df.dropna(axis=1,how='all',inplace=True)\n",
    "        except:\n",
    "            raise Exception(str(study_accession) + ' is not a valid project or study number for ' + mode)\n",
    "    elif mode == 'sra':\n",
    "        p1 = subprocess.Popen(['esearch', '-db', 'sra', '-query', study_accession],\n",
    "                          stdout=subprocess.PIPE)\n",
    "        p2 = subprocess.Popen(['efetch', '-format', 'runinfo'], stdin=p1.stdout,\n",
    "                          stdout=subprocess.PIPE)\n",
    "        count=0\n",
    "        headers=[]\n",
    "        for i in p2.stdout:\n",
    "            if count == 0:\n",
    "                sra_headers = i.decode(\"utf-8\").replace('\\n','').split(',')\n",
    "                for h in sra_headers:\n",
    "                    headers.append(scrub_special_chars(split_caps(h).lower(),custom_dict={'i_d':'id',\n",
    "                                                                                    's_r_a':'sra',\n",
    "                                                                                    'e_b_i':'ebi',\n",
    "                                                                                    's_r_r':'srr',\n",
    "                                                                                    'm_b':'mb'}))\n",
    "                    study_df=pd.DataFrame({},columns=headers)\n",
    "            else:\n",
    "                tmp=str(i.decode(\"utf-8\"))\n",
    "                \n",
    "                if len(tmp) > 1:\n",
    "                    a_series = pd.Series(list(csv.reader(tmp.splitlines(),delimiter=','))[0], index = df.columns)\n",
    "                    study_df = study_df.append(a_series, ignore_index=True)\n",
    "            count += 1\n",
    "    else:\n",
    "        raise Exception(mode + \" is not a valid repository.\")\n",
    "    \n",
    "    create_details_file(study_df,study_accession,mode)\n",
    "    return study_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new feed in study_df above\n",
    "def add_sample_info(input_df,mode='ebi',plat=[],strat=[],validator_files={},sample_type_column='scientific_name'):\n",
    "    if mode =='ebi':\n",
    "        identifier = 'secondary_sample_accession'\n",
    "        run_accession = 'run_accession'        \n",
    "        input_df['platform']=input_df['instrument_platform']\n",
    "    elif mode == 'sra':\n",
    "        identifier = 'sample'\n",
    "        run_accession = 'run'\n",
    "        lib_strategy='library_source'\n",
    "        input_df['instrument_model']=input_df['model']\n",
    "    else:\n",
    "        raise Exception(mode + \" is not a valid repository.\")\n",
    "        \n",
    "    #Note: for now this loop just uses the data in EBI since it is mirrored with NCBI    \n",
    "    sample_info_list=[]\n",
    "    sample_count_dict = {}\n",
    "    prep_df_dict={}\n",
    "    \n",
    "    #apply filters for platforms and strategies\n",
    "    if len(plat) > 0:\n",
    "        for p in plat:\n",
    "            input_df = input_df[input_df['platform'] != p]\n",
    "    if len(plat) > 0:\n",
    "        for s in strat:\n",
    "            input_df = input_df[input_df['library_strategy'] != s]\n",
    "            \n",
    "    for index, row in input_df.iterrows():\n",
    "        sample_accession = row[identifier]\n",
    "        prep_type = row['library_strategy']\n",
    "        if prep_type not in sample_count_dict.keys() :\n",
    "            sample_count_dict[prep_type]= {sample_accession:0}\n",
    "        elif sample_accession not in sample_count_dict[prep_type].keys():\n",
    "            sample_count_dict[prep_type][sample_accession]= 0\n",
    "        else:\n",
    "            sample_count_dict[prep_type][sample_accession] = sample_count_dict[prep_type][sample_accession] + 1\n",
    "\n",
    "        sampleUrl = \"http://www.ebi.ac.uk/ena/data/view/\" + sample_accession \\\n",
    "                + \"&display=xml\"\n",
    "        if DEBUG: logger.info(sampleUrl)\n",
    "        response = requests.get(sampleUrl)\n",
    "        xml_dict=parse(response.content)\n",
    "        input_df.at[index,'sample_title_specific']=xml_dict['ROOT']['SAMPLE']['TITLE']\n",
    "        sn=xml_dict['ROOT']['SAMPLE']['SAMPLE_NAME']\n",
    "        for s in sn.keys():\n",
    "            input_df.at[index,s]=sn[s]\n",
    "\n",
    "        sa=xml_dict['ROOT']['SAMPLE']['SAMPLE_ATTRIBUTES']['SAMPLE_ATTRIBUTE']\n",
    "        for s in sa:\n",
    "            input_df.at[index,s['TAG']]=s['VALUE']\n",
    "        input_df.at[index,'prep_file']=prep_type + '_' + str(sample_count_dict[prep_type][sample_accession])\n",
    "\n",
    "    #need to catch common issue where secondary_sample_accession is identical for unique samples\n",
    "    #for now assume that in this case, libarary_name will be unique\n",
    "    input_df['sample_name']=input_df[identifier]\n",
    "    if len(input_df) > 1 and input_df[identifier].nunique() != 1:\n",
    "        if input_df['library_name'].nunique() != 1:\n",
    "            input_df['sample_name']=input_df['library_name']\n",
    "        else:\n",
    "            input_df['sample_name']=input_df[identifier]+ '.' + input_df[run_accession]\n",
    "\n",
    "    input_df['run_prefix']=input_df[run_accession]\n",
    "    \n",
    "    output_df=validate_samples(input_df,sample_type_column,validator_files)\n",
    "    #tidy input_df before merging\n",
    "    #input_df.columns = [split_caps(col) for col in input_df.columns]\n",
    "    input_df.columns = [scrub_special_chars(col).lower() for col in input_df.columns]\n",
    "        \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_samples(raw_df,sample_type_col,yaml_validator_dict):\n",
    "    st_list = []\n",
    "    for st in raw_df[sample_type_col].unique():\n",
    "        df_to_validate = raw_df[raw_df[sample_type_col]==st]\n",
    "        if st not in yaml_validator_dict.keys():\n",
    "            logger.warning(\"No yaml file for validating \" + st + \" You may provide one or more custom files \" +\n",
    "                  \" using the --validators or --yaml_dir flag.\")\n",
    "        else:\n",
    "            with open(yaml_validator_dict[sample_type]) as file:\n",
    "                validator_yaml = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "            for k in validator_yaml.keys():\n",
    "                if k not in df_to_validate.columns:\n",
    "                    msg=k + ' not found in columns.' \n",
    "                    try:\n",
    "                        df_to_validate[k]= validator_yaml[k]['default']\n",
    "                        msg = msg + \"Setting \" + k + \" to \" + validator_yaml[k]['default'] + \" for \" + st + \"samples\"                        \n",
    "                    except:\n",
    "                        df_to_validate[k]= 'not provided'\n",
    "                        msg = msg + k + \" has no default, will be encoded as 'not provided'\"\n",
    "                    logger.warning(msg)\n",
    "                else:\n",
    "                    #construct rules\n",
    "                    uniq = df_to_validate[k].unique()\n",
    "                    allowed_list = []\n",
    "                    min_value= ''\n",
    "                    max_value = ''\n",
    "                    min_value_excl= ''\n",
    "                    max_value_excl = ''\n",
    "                    if 'anyof' in validator_yaml[k].keys():\n",
    "                        anyof_list = validator_yaml[k]['anyof']            \n",
    "                        for r in anyof_list:\n",
    "                            if r['type'] == 'string':\n",
    "                                for a in r['allowed']:\n",
    "                                    allowed_list.append(a)\n",
    "                            elif r['type'] == 'number':\n",
    "                                if 'min' in r.keys():\n",
    "                                    min_value = r['min']\n",
    "                                if 'max' in r.keys():\n",
    "                                    max_value = r['max']\n",
    "                                if 'min_exclusive' in r.keys():\n",
    "                                    min_value = r['min']\n",
    "                                if 'max_exclusive' in r.keys():\n",
    "                                    max_value = r['max']\n",
    "                    elif validator_yaml[k]['type'] in validator_yaml[k].keys():\n",
    "                        if validator_yaml[k]['type']== 'string':\n",
    "                            allowed_list=validator_yaml[k]['allowed']\n",
    "                        if validator_yaml[k]['type'] == 'number' or validator_yaml[k]['type'] =='integer':\n",
    "                            if 'min' in validator_yaml[k].keys():\n",
    "                                min_value = validator_yaml[k]['min']\n",
    "                            if 'max' in validator_yaml[k].keys():\n",
    "                                max_value = validator_yaml[k]['max']\n",
    "                            if 'min_exclusive' in validator_yaml[k].keys():\n",
    "                                min_value_excl = validator_yaml[k]['min']\n",
    "                            if 'max_exclusive' in validator_yaml[k].keys():\n",
    "                                max_value_excl = validator_yaml[k]['max']\n",
    "                    \n",
    "                    #alert user of issues\n",
    "                    for u in uniq:\n",
    "                        if not u.isnumeric():\n",
    "                            if u not in allowed_list and len(allowed_list) > 0:\n",
    "                                logger.warning(\"Warning \" + u + \" found in column \" + k + \" but not allowed per Qiimp template. \" \n",
    "                                     + \"valid values: \" + str(allowed_list))          \n",
    "                        else:\n",
    "                            if u not in allowed_list: #assume it's actually a number\n",
    "                                if min_value != '' and u < min_value:\n",
    "                                    logger.warning(\"Warning \" + u + \" found in column \" + k + \" but less than min value per Qiimp template: \"\n",
    "                                         + str(min_value))\n",
    "                                if max_value != '' and u > max_value:\n",
    "                                    logger.warning(\"Warning \" + u + \" found in column \" + k + \" but more than max value per Qiimp template: \"\n",
    "                                         + str(max_value))\n",
    "                                if min_value_excl != '' and u <= min_value_excl:\n",
    "                                    logger.warning(\"Warning \" + u + \" found in column \" + k + \" but less than min value per Qiimp template: \"\n",
    "                                         + str(min_value_excl))\n",
    "                                if max_value_excl != '' and u >= max_value_excl:    \n",
    "                                    logger.warning(\"Warning \" + u + \" found in column \" + k + \" but not allowed per Qiimp template: \"\n",
    "                                         + str(max_value_excl))\n",
    "        st_list.append(df_to_validate)\n",
    "    valid_df = pd.concat(st_list)\n",
    "    return valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write files out and then do checks, e.g. always debug mode\n",
    "def create_details_file(study_details_df, study_accession,mode='ebi',file_suffix=\"_detail\"):\n",
    "    \"\"\"Returns the details of the EBI/SRA study\n",
    "\n",
    "    If the accession ID is valid, generate a .details.txt, and return the\n",
    "    detail file name of this EBI/SRA study. Else return None\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    study_accession : string\n",
    "        The accession ID of the EBI/SRA study\n",
    "\n",
    "    file_suffix : string\n",
    "        The suffix for the output study detail file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        study details file name\n",
    "    \"\"\"\n",
    "    study_details = study_accession + \"_\" + mode + file_suffix + \".txt\"\n",
    "    study_details_df.to_csv(study_details,sep='\\t',header=True,index=False)\n",
    "    return study_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_info_files(final_df,prefix=''):\n",
    "    prep_info_columns = ['run_prefix','experiment_accession','platform','instrument_model','library_strategy','library_source','library_layout','library_selection','fastq_ftp']\n",
    "    final_df.columns =[scrub_special_chars(col).lower() for col in final_df.columns]\n",
    "    if DEBUG: logger.info(final_df.columns)\n",
    "    #write sample_info\n",
    "    sample_df=final_df[final_df.columns[~final_df.columns.isin(prep_info_columns)]].set_index('sample_name',inplace=False)\n",
    "    sample_df=sample_df.dropna(axis=1,how='all')\n",
    "    sample_df.to_csv(prefix+'_sample_info.tsv',index=True,index_label='sample_name')\n",
    "    for prep_file in final_df['prep_file']:\n",
    "        prep_df = final_df[final_df['prep_file']==prep_file]\n",
    "        prep_info_columns.append('sample_name')\n",
    "        prep_df= prep_df[prep_info_columns].set_index('sample_name',inplace=False)\n",
    "        prep_df=prep_df.dropna(axis=1,how='all')\n",
    "        prep_df.to_csv(prefix+'_prep_info_'+ prep_file + '.tsv',index=True,index_label='sample_name')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sequencing_data(download_df,output_dir=\"./\",mode='ebi'):\n",
    "    \"\"\"Fetch all the meta file(s) for EBI study\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    study_accession : string\n",
    "        The accession ID of the EBI study\n",
    "\n",
    "    study_details : string\n",
    "        study detail file name\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Downloading the fastqs\")\n",
    "    # Download the fastqs\n",
    "    \n",
    "    for index, row in download_df.iterrows():\n",
    "        if mode == 'ebi':\n",
    "            files = row['fastq_ftp'].split(';')\n",
    "            for f in files:\n",
    "                fq_path = output_dir + \"/\" + f.split('/')[-1]\n",
    "                if DEBUG: logger.info(f)\n",
    "                if type(f) != str:\n",
    "                    logger.warning(\"Skipping sample:\" + row['sample_name']\n",
    "                                   + \", run: \" + row['run_accession'] + \"No fastq ftp found.\")      \n",
    "                elif path.isfile(fq_path):\n",
    "                    logger.warning(\"Skipping \" + fq_path)\n",
    "                    logger.warning(\"File exists\")\n",
    "                else:\n",
    "                    if not path.exists(output_dir):\n",
    "                        makedirs(output_dirh)\n",
    "                    urlretrieve(\"ftp://\" +f, fq_path)\n",
    "        elif mode =='sra':\n",
    "            subprocess.run(['fastq-dump', '-I', '--split-files', '--gzip', row['run_accession']])\n",
    "        else:\n",
    "            raise Exception(mode + \" is not a valid repository\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-o OUTPUT] [-mode {ebi,sra}]\n",
      "                             [-prefix PREFIX]\n",
      "                             [-strat [{POOLCLONE,CLONE,CLONEEND,WGS,WGA,WCS,WXS,AMPLICON,ChIP-Seq,RNA-Seq,MRE-Seq,MeDIP-Seq,MBD-Seq,MNase-Seq,DNase-Hypersensitivity,Bisulfite-Seq,EST,FL-cDNA,miRNA-Seq,ncRNA-Seq,FINISHING,TS,Tn-Seq,VALIDATION,FAIRE-seq,SELEX,RIP-Seq,ChIA-PET,RAD-Seq} [{POOLCLONE,CLONE,CLONEEND,WGS,WGA,WCS,WXS,AMPLICON,ChIP-Seq,RNA-Seq,MRE-Seq,MeDIP-Seq,MBD-Seq,MNase-Seq,DNase-Hypersensitivity,Bisulfite-Seq,EST,FL-cDNA,miRNA-Seq,ncRNA-Seq,FINISHING,TS,Tn-Seq,VALIDATION,FAIRE-seq,SELEX,RIP-Seq,ChIA-PET,RAD-Seq} ...]]]\n",
      "                             [-plat [{LS454,Illumina,Ion Torrent,PacBio_SMRT,OXFORD_NANOPORE} [{LS454,Illumina,Ion Torrent,PacBio_SMRT,OXFORD_NANOPORE} ...]]]\n",
      "                             [-yaml [VALIDATORS [VALIDATORS ...]]]\n",
      "                             [-yaml_dir VALIDATOR_DIR] [-no-seqs] [-sep SEP]\n",
      "                             [-v]\n",
      "                             [P [P ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adswafford/miniconda3/envs/ebi_sra_importer/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # parse the flags and initialize output file names\n",
    "    # TODO:Reword help info\n",
    "    parser = ArgumentParser(description='Please note that the following ' +\n",
    "                            'packages have to be installed for running this ' +\n",
    "                            'script: 1)lxml 2)pandas 3)glob 4)csv 5)sys ' +\n",
    "                            '6)urllib 7)argparse 8)requests 9)xmltodict ' +\n",
    "                            '10)subprocess 11)bioconda 12)sra-tools 13)os ' +\n",
    "                            '14)entrez-direct 15) pyyaml 16) xlrd')\n",
    "    parser.add_argument(\"project\", metavar=\"P\", type=str, nargs='*',help=\"EBI/ENA project or study accession(s) \" +\n",
    "                        \"to retrieve\")\n",
    "    parser.add_argument(\"-o\",\"--output\", default='./',help='directory for output files. Default is working directory')\n",
    "    parser.add_argument(\"-mode\", \"--mode\", default='ebi', help=\"sra accession \" +\n",
    "                        \"repository to be queried\", choices=['ebi','sra'])\n",
    "    parser.add_argument(\"-prefix\", \"--prefix\", default='', help=\"prefix to prepend to output info files\")\n",
    "    parser.add_argument(\"-strat\",\"--strategies\",nargs='*',choices=['POOLCLONE','CLONE','CLONEEND','WGS','WGA',\n",
    "                                                       'WCS','WXS','AMPLICON','ChIP-Seq','RNA-Seq',\n",
    "                                                       'MRE-Seq','MeDIP-Seq','MBD-Seq','MNase-Seq',\n",
    "                                                       'DNase-Hypersensitivity','Bisulfite-Seq','EST',\n",
    "                                                       'FL-cDNA','miRNA-Seq','ncRNA-Seq','FINISHING',\n",
    "                                                       'TS','Tn-Seq','VALIDATION','FAIRE-seq','SELEX',\n",
    "                                                       'RIP-Seq','ChIA-PET','RAD-Seq'],\n",
    "                        help=\"list of one or more libary strategies to restrict selection.\")\n",
    "    parser.add_argument(\"-plat\", \"--platforms\", nargs='*', choices=['LS454','Illumina','Ion Torrent','PacBio_SMRT',\n",
    "                                                                                        'OXFORD_NANOPORE'],\n",
    "                        help=\"List of one or more platforms to restrict selection.\")\n",
    "    parser.add_argument(\"-yaml\",\"--validators\",nargs='*', help=\"one or more yaml files in QIIMP format for validation.\")\n",
    "    parser.add_argument(\"-yaml-dir\",\"--yaml_dir\", default ='./', help=\"one or more yaml files in QIIMP format for validation.\")\n",
    "    parser.add_argument(\"-no-seqs\", \"--no_seqs\", default=False,action='store_true', help=\"Omit download of fastq files\")\n",
    "    parser.add_argument(\"-sep\",\"--sep\",default=';',help=\"separator for parsing description, default is ';' \")\n",
    "    parser.add_argument(\"-v\", \"--verbose\", default=False, action='store_true', help=\"Output additional messages\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.project is None:\n",
    "        logger.warning(\"\"\"\n",
    "                python EBI_SRA_Downloader.py [accession] [accession ... N]\n",
    "                    Generate the study info, study detail, prep, and  sample\n",
    "                    files for the entered EBI accession, and download the\n",
    "                    FASTQ files.\n",
    "                Optional flags:\n",
    "                    -output [directory where files will be saved]\n",
    "                    -mode [specifies which repository to use]                    \n",
    "                    -prefix [prefix for sample and prep info files]\n",
    "                    --strategy [list of one or more library strategies to select]\n",
    "                    --platforms [list of one or more sequencing platforms to select]\n",
    "                    --validators [list of one or more yaml files to use in validating]\n",
    "                    --no_seqs [skip downloading files]\n",
    "                    --verbose          \n",
    "                    --sep [provide a delimiter for parsing the description field]\n",
    "               \"\"\")\n",
    "        sys.exit(2)\n",
    "    else:\n",
    "        #settings\n",
    "        mode=args.mode\n",
    "        sep= args.sep\n",
    "        DEBUG = args.verbose\n",
    "        omit_seqs = args.no_seqs\n",
    "        \n",
    "        #set up logging\n",
    "        handler = logging.StreamHandler()\n",
    "        fmt_str = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    "        handler.setFormatter(logging.Formatter(fmt_str))\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.addHandler(handler)\n",
    "                \n",
    "        fh=logging.FileHandler(args.log)\n",
    "        logger.addHandler(fh)\n",
    "        if DEBUG: logger.setLevel(logging.INFO)\n",
    "            \n",
    "        #set output directory\n",
    "        output = args.output\n",
    "        \n",
    "        if list(output)[-1] != '/':\n",
    "            output = output + '/'\n",
    "        prefix = output + args.prefix \\\n",
    "        \n",
    "        #set up validators\n",
    "        yaml_validator_dict = {}\n",
    "        yaml_list =  []\n",
    "        \n",
    "        if args.validators is not None:\n",
    "            for y in args.validators:\n",
    "                yaml_list.append(y)\n",
    "        else:\n",
    "            for file in os.listdir(args.yaml_dir):\n",
    "                if file.endswith(\".yml\"):\n",
    "                    yaml_list.append(os.path.join(args.yaml_dir, file))\n",
    "                      \n",
    "        yaml_validator_dict = add_yaml_validators(yaml_list)\n",
    "                       \n",
    "        platforms=[]\n",
    "        if args.platforms is not None:\n",
    "            for p in args.platforms:\n",
    "                platforms.append(p)\n",
    "        \n",
    "        strategies =[]\n",
    "        if args.strategies is not None:\n",
    "            for s in args.strategies:\n",
    "                strategies.append(s)\n",
    "               \n",
    "        # Retreive study information\n",
    "        for p in args.project:\n",
    "            study = get_study_details(p,mode)\n",
    "            \n",
    "            #tidy metadata\n",
    "            md=add_sample_info(study,platforms,strategies,yaml_validator_dict)\n",
    "            \n",
    "            #write out files\n",
    "            file_prefix = prefix + '_' + p\n",
    "            write_info_files(md,file_prefix)\n",
    "            \n",
    "            if not omit_seqs:\n",
    "                fetch_sequencing_data(md,output,mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
