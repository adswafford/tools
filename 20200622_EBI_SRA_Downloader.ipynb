{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries used\n",
    "import csv\n",
    "import sys\n",
    "import glob\n",
    "import logging\n",
    "import requests\n",
    "import subprocess\n",
    "from xmltodict import parse\n",
    "from lxml import etree\n",
    "import os\n",
    "from os import path, makedirs\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "from pandas import read_csv, DataFrame\n",
    "import re\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set constants\n",
    "handler = logging.StreamHandler()\n",
    "fmt_str = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    "handler.setFormatter(logging.Formatter(fmt_str))\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "#need to work this out to write log\n",
    "#fh = logging.FileHandler('test.log')\n",
    "#logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_caps(cap_string):\n",
    "    temp_list = []\n",
    "    temp_list = re.findall('[A-Z][^A-Z]*', cap_string)\n",
    "    if len(temp_list) > 0:\n",
    "        return \"_\".join(temp_list)\n",
    "    else:\n",
    "        return cap_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_special_chars(input_string,custom_dict={}):\n",
    "    replace_dict ={\"__\":\"_\",\n",
    "                   \" \":\"_\",\n",
    "                   \"-\": \"_\",\n",
    "                   \"(\": \"_leftparen_\",\n",
    "                   \")\": \"_rightparen_\",\n",
    "                   \"/\": \"_per_\",\n",
    "                   \"-\":\"_\",\n",
    "                   \"|\":\"_bar_\",\n",
    "                   \"~\":\"_tilde_\",\n",
    "                   \"`\":\"_\",\n",
    "                   \"@\":\"_at_\",\n",
    "                   \"#\":\"_number_\",\n",
    "                   \"$\":\"dollar\",\n",
    "                   \"%\":\"_perc_\",\n",
    "                   \"^\":\"_carrot_\",\n",
    "                   \"&\":\"_and_\",\n",
    "                   \"*\":\"_star_\",\n",
    "                   \"+\":\"_plus\",\n",
    "                   \"=\":\"_equals\",\n",
    "                   \"\\\\\":\"_per_\",\n",
    "                   \"{\":\"_leftbracket_\",\n",
    "                   \"}\":\"_rightbracket_\",\n",
    "                   \"[\":\"_leftbracket_\",\n",
    "                   \"]\":\"_rightbracket_\",\n",
    "                   \"?\":\"_question\",\n",
    "                   \"<\":\"_less_\",\n",
    "                   \">\":\"_more_\",\n",
    "                   \",\":\"_\",\n",
    "                   \".\":\"_\",}\n",
    "    for k in replace_dict.keys():\n",
    "        input_string=input_string.replace(k,replace_dict[k])\n",
    "    for k in custom_dict.keys():\n",
    "        input_string=input_string.replace(k,custom_dict[k])\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qiimp_parser(filename):\n",
    "    ext=Path(filename).suffix\n",
    "    parsed_yml={}\n",
    "    if ext == '.xlsx':\n",
    "        try:\n",
    "            temp_yml=pd.read_excel(filename,sheet_name='metadata_schema',header=None) #assume QIIMP-style excel\n",
    "            parsed_yml = yaml.load(temp_yml.at[0,0])\n",
    "        except:\n",
    "            logger.warning(\"Invalid .xlsx file. Please ensure file is from QIIMP or contains a compliant yaml \" + \n",
    "                          \"in cell A1 of the sheet labelled 'metadata_schema'.\")\n",
    "    elif ext == '.yml' or ext == '.yaml':\n",
    "        with open(filename) as file:\n",
    "            # The FullLoader parameter handles the conversion from YAML\n",
    "            # scalar values to Python the dictionary format\n",
    "            parsed_yml=yaml.load(file, Loader=yaml.FullLoader) \n",
    "            if DEBUG: logger.info(parsed_yml)\n",
    "    else:\n",
    "        logger.warning(\"Invalid file extension for yaml parsing: \" + str(ext))\n",
    "    \n",
    "    if len(parsed_yml) == 0:\n",
    "        logger.warning(\"The file \" + filename +\" contains no yaml data. Please check contents and try again.\")\n",
    "    \n",
    "    return parsed_yml             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_yaml_validators(validators,field='scientific_name'):\n",
    "    yaml_dict={}\n",
    "    for v in validators:\n",
    "        new_yaml={}\n",
    "        try:\n",
    "            new_yaml = qiimp_parser(v)\n",
    "        except:\n",
    "            logger.warning(\"Could not open yaml file \" + v + \" Please ensure the file exists and is a valid yaml file.\")\n",
    "        \n",
    "        if field not in new_yaml.keys():\n",
    "            logger.warning(\"Invalid validator yaml. Please ensure a default value is provided for '\" + field +\n",
    "                      \"'. Available keys in file: \" + str(new_yaml.keys()))\n",
    "        elif 'default' not in new_yaml[field].keys():\n",
    "            logger.warning(\"Invalid validator yaml. Please ensure a default value is provided for '\" + field +\n",
    "                      \"'. Available keys in : \" + field + str(new_yaml[field].keys()))\n",
    "        else:\n",
    "            yaml_dict[new_yaml[field]['default']] = v\n",
    "            \n",
    "    return yaml_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these functions create the study_details files for ebi and sra respectively\n",
    "\n",
    "def get_study_details(study_accession,mode='ebi'):\n",
    "    \n",
    "    if mode == 'ebi':        \n",
    "        host = \"http://www.ebi.ac.uk/ena/data/warehouse/filereport?accession=\"\n",
    "        read_type = \"&result=read_run&\"\n",
    "        fields = \"library_name,secondary_sample_accession,run_accession,\" + \\\n",
    "                 \"experiment_accession,fastq_ftp,library_source,\" + \\\n",
    "                 \"instrument_platform,submitted_format,library_strategy,\" +\\\n",
    "                 \"library_layout,tax_id,scientific_name,instrument_model,\" + \\\n",
    "                \"library_selection,center_name,experiment_title,\" +\\\n",
    "                \"study_title,study_alias,experiment_alias,sample_alias,sample_title\"\n",
    "        url = ''.join([host, study_accession, read_type, \"fields=\", fields])\n",
    "        if DEBUG: logger.info(url)\n",
    "        try:\n",
    "            study_df = pd.read_csv(url,sep='\\t')\n",
    "            study_df.dropna(axis=1,how='all',inplace=True)\n",
    "        except:\n",
    "            raise Exception(str(study_accession) + ' is not a valid project or study number for ' + mode)\n",
    "    elif mode == 'sra':\n",
    "        p1 = subprocess.Popen(['esearch', '-db', 'sra', '-query', study_accession],\n",
    "                          stdout=subprocess.PIPE)\n",
    "        p2 = subprocess.Popen(['efetch', '-format', 'runinfo'], stdin=p1.stdout,\n",
    "                          stdout=subprocess.PIPE)\n",
    "        count=0\n",
    "        headers=[]\n",
    "        for i in p2.stdout:\n",
    "            if count == 0:\n",
    "                sra_headers = i.decode(\"utf-8\").replace('\\n','').split(',')\n",
    "                for h in sra_headers:\n",
    "                    headers.append(scrub_special_chars(split_caps(h).lower(),custom_dict={'i_d':'id',\n",
    "                                                                                    's_r_a':'sra',\n",
    "                                                                                    'e_b_i':'ebi',\n",
    "                                                                                    's_r_r':'srr',\n",
    "                                                                                    'm_b':'mb'}))\n",
    "                    study_df=pd.DataFrame({},columns=headers)\n",
    "            else:\n",
    "                tmp=str(i.decode(\"utf-8\"))\n",
    "                \n",
    "                if len(tmp) > 1:\n",
    "                    a_series = pd.Series(list(csv.reader(tmp.splitlines(),delimiter=','))[0], index = df.columns)\n",
    "                    study_df = study_df.append(a_series, ignore_index=True)\n",
    "            count += 1\n",
    "    else:\n",
    "        raise Exception(mode + \" is not a valid repository.\")\n",
    "    \n",
    "    create_details_file(study_df,study_accession,mode)\n",
    "    return study_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now feed in study_df above\n",
    "def get_sample_info(input_df,mode='ebi',plat=[],strat=[],validator_files={},prefix='',names=[],sample_type_column='scientific_name'):\n",
    "    \n",
    "    #check to see if the .part file already exists:\n",
    "    pre_validation_file = prefix + \"_unvalidated_sample_info.part\"\n",
    "    if not path.isfile(pre_validation_file):\n",
    "        if mode =='ebi':\n",
    "            identifier = 'secondary_sample_accession'\n",
    "            run_accession = 'run_accession'        \n",
    "            input_df['platform']=input_df['instrument_platform']\n",
    "        elif mode == 'sra':\n",
    "            identifier = 'sample'\n",
    "            run_accession = 'run'\n",
    "            lib_strategy='library_source'\n",
    "            input_df['instrument_model']=input_df['model']\n",
    "        else:\n",
    "            raise Exception(mode + \" is not a valid repository.\")\n",
    "\n",
    "        #Note: for now this loop just uses the data in EBI since it is mirrored with NCBI    \n",
    "        sample_info_list=[]\n",
    "        sample_count_dict = {}\n",
    "        prep_df_dict={}\n",
    "\n",
    "        #apply filters for platforms and strategies\n",
    "        except_msg=''\n",
    "        if len(plat) > 0:\n",
    "            except_msg = except_msg + \"Selected Platforms: \" + str(plat) + \"\\n Available Platforms:\" +\\\n",
    "                        str(input_df['platform'].str.lower().unique()) + \"\\n\"\n",
    "            input_df = input_df[input_df['platform'].str.lower().isin(plat)]\n",
    "            \n",
    "        if len(strat) > 0:\n",
    "            except_msg = except_msg + \"Selected Strategies: \" + str(strat) + \"\\n Available Strategies:\" +\\\n",
    "                        str(input_df['library_strategy'].str.lower().unique()) + \"\\n\"\n",
    "            input_df = input_df[input_df['library_strategy'].str.lower().isin(strat)]\n",
    "            \n",
    "        if len(names) > 0:\n",
    "            except_msg = except_msg + \"Selected scientific names: \" + str(names) + \"\\n Available Scientific Names:\" +\\\n",
    "                        str(input_df['scientific_name'].str.lower().unique()) + \"\\n\"\n",
    "            input_df = input_df[input_df['scientific_name'].str.lower().isin(names)]\n",
    "        \n",
    "        if len(input_df) == 0:\n",
    "            raise Exception(\"No files after selection criteria:\\n\" + except_msg)\n",
    "        \n",
    "        for index, row in input_df.iterrows():\n",
    "            sample_accession = row[identifier]\n",
    "            prep_type = row['library_strategy']\n",
    "            if prep_type not in sample_count_dict.keys() :\n",
    "                sample_count_dict[prep_type]= {sample_accession:0}\n",
    "            elif sample_accession not in sample_count_dict[prep_type].keys():\n",
    "                sample_count_dict[prep_type][sample_accession]= 0\n",
    "            else:\n",
    "                sample_count_dict[prep_type][sample_accession] = sample_count_dict[prep_type][sample_accession] + 1\n",
    "\n",
    "            sampleUrl = \"http://www.ebi.ac.uk/ena/data/view/\" + sample_accession \\\n",
    "                    + \"&display=xml\"\n",
    "            if DEBUG: logger.info(sampleUrl)\n",
    "\n",
    "            response = requests.get(sampleUrl)\n",
    "            xml_dict=parse(response.content)\n",
    "            input_df.at[index,'sample_title_specific']=xml_dict['ROOT']['SAMPLE']['TITLE']\n",
    "\n",
    "            sn=xml_dict['ROOT']['SAMPLE']['SAMPLE_NAME']\n",
    "            for s in sn.keys():\n",
    "                col = scrub_special_chars(s).lower()\n",
    "                input_df.at[index,col]=sn[s]\n",
    "\n",
    "            sa=xml_dict['ROOT']['SAMPLE']['SAMPLE_ATTRIBUTES']['SAMPLE_ATTRIBUTE']\n",
    "            for s in sa:\n",
    "                col = scrub_special_chars(s['TAG']).lower()\n",
    "                input_df.at[index,col]=s['VALUE']\n",
    "            input_df.at[index,'prep_file']=prep_type + '_' + str(sample_count_dict[prep_type][sample_accession])\n",
    "\n",
    "        #set sample_name based on identifier column\n",
    "        input_df['sample_name']=input_df[identifier]\n",
    "\n",
    "        #need to catch common issue where secondary_sample_accession is identical for unique samples\n",
    "        #for now assume that in this case, libarary_name will be unique, and if it isn't combined sample and run names\n",
    "        if len(input_df) > 1 and input_df[identifier].nunique() == 1:\n",
    "            if input_df['library_name'].nunique() != 1:\n",
    "                #print(str(len(_input_df)) + \" is length and input_df[identifier].nunique() = \" + str(input_df[identifier].nunique()))\n",
    "                input_df['sample_name']=input_df['library_name']\n",
    "            else:\n",
    "                input_df['sample_name']=input_df[identifier]+ '.' + input_df[run_accession]\n",
    "\n",
    "        input_df['run_prefix']=input_df[run_accession]\n",
    "\n",
    "        #the loop above takes the most time so write out a placeholder file for faster re-running if interrupted\n",
    "        input_df.to_csv(pre_validation_file,sep='\\t',index=False)    \n",
    "    else:\n",
    "        input_df = pd.read_csv(pre_validation_file,sep='\\t',dtype=str)\n",
    "        \n",
    "    output_df=validate_samples(input_df,sample_type_column,validator_files,prefix)\n",
    "    #tidy output before returning\n",
    "    output_df.columns = [scrub_special_chars(col).lower() for col in output_df.columns]\n",
    "        \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_samples(raw_df,sample_type_col,yaml_validator_dict,prefix):\n",
    "    st_list = []\n",
    "    msg = ''\n",
    "    for st in raw_df[sample_type_col].unique():\n",
    "        df_to_validate = raw_df[raw_df[sample_type_col]==st]\n",
    "        if force:\n",
    "            validator_yaml = yaml.load(open(yaml_validator_dict[0]), Loader=yaml.FullLoader)\n",
    "            if 'scientific_name' in df_to_validate.keys():\n",
    "                df_to_validate=df_to_validate.rename({'scientific_name':'orig_scientific_name'},axis=1)\n",
    "        else:\n",
    "            if st not in yaml_validator_dict.keys():\n",
    "                logger.warning(\"No yaml file for validating \" + st + \" You may provide one or more custom files \" +\n",
    "                  \" using the --validators flag.\")\n",
    "            else:\n",
    "                validator_yaml = yaml.load(open(yaml_validator_dict[st]), Loader=yaml.FullLoader)\n",
    "           \n",
    "        for k in validator_yaml.keys():\n",
    "            if k not in df_to_validate.columns:\n",
    "                msg= msg + k + ' not found in metadata.\\n' \n",
    "                try:\n",
    "                    df_to_validate[k]= validator_yaml[k]['default']\n",
    "                    msg = msg + \"Setting \" + k + \" to \" + validator_yaml[k]['default'] + \" for \" + st + \" samples\\n\"                     \n",
    "                except:\n",
    "                    df_to_validate[k]= 'not provided'\n",
    "                    msg = msg + k + \" has no default in yaml template. Encoding as 'not provided'\\n\"\n",
    "            else:\n",
    "                #construct rules\n",
    "                uniq = df_to_validate[k].unique()\n",
    "                allowed_list = []\n",
    "                min_value= ''\n",
    "                max_value = ''\n",
    "                min_value_excl= ''\n",
    "                max_value_excl = ''\n",
    "                if 'anyof' in validator_yaml[k].keys():\n",
    "                    anyof_list = validator_yaml[k]['anyof']            \n",
    "                    for r in anyof_list:\n",
    "                        if r['type'] == 'string':\n",
    "                            for a in r['allowed']:\n",
    "                                allowed_list.append(a)\n",
    "                        elif r['type'] == 'number':\n",
    "                            if 'min' in r.keys():\n",
    "                                min_value = r['min']\n",
    "                            if 'max' in r.keys():\n",
    "                                max_value = r['max']\n",
    "                            if 'min_exclusive' in r.keys():\n",
    "                                min_value = r['min']\n",
    "                            if 'max_exclusive' in r.keys():\n",
    "                                max_value = r['max']\n",
    "                elif validator_yaml[k]['type'] in validator_yaml[k].keys():\n",
    "                    if validator_yaml[k]['type']== 'string':\n",
    "                        allowed_list=validator_yaml[k]['allowed']\n",
    "                    if validator_yaml[k]['type'] == 'number' or validator_yaml[k]['type'] =='integer':\n",
    "                        if 'min' in validator_yaml[k].keys():\n",
    "                            min_value = validator_yaml[k]['min']\n",
    "                        if 'max' in validator_yaml[k].keys():\n",
    "                            max_value = validator_yaml[k]['max']\n",
    "                        if 'min_exclusive' in validator_yaml[k].keys():\n",
    "                            min_value_excl = validator_yaml[k]['min']\n",
    "                        if 'max_exclusive' in validator_yaml[k].keys():\n",
    "                            max_value_excl = validator_yaml[k]['max']\n",
    "\n",
    "                #alert user of issues\n",
    "                for u in uniq:\n",
    "                    if not u.isnumeric():\n",
    "                        if u not in allowed_list and len(allowed_list) > 0:\n",
    "                            msg = msg + \"Warning \" + u + \" found in column \" + k + \" but not allowed per Qiimp template.\" +\\\n",
    "                            \"valid values: \" + str(allowed_list) + \"\\n\"          \n",
    "                    else:\n",
    "                        if u not in allowed_list: #assume it's actually a number\n",
    "                            if min_value != '' and u < min_value:\n",
    "                                msg = msg + \"Warning \" + u + \" found in column \" + k + \" but less than min value per yaml: \" +\\\n",
    "                                str(min_value) + \"\\n\"\n",
    "                            if max_value != '' and u > max_value:\n",
    "                                msg = msg + \"Warning \" + u + \" found in column \" + k + \" but more than max value per yaml: \" +\\\n",
    "                                     str(max_value) + \"\\n\"\n",
    "                            if min_value_excl != '' and u <= min_value_excl:\n",
    "                                msg = msg + \"Warning \" + u + \" found in column \" + k + \" but less than min value per yaml: \" +\\\n",
    "                                     str(min_value_excl) + \"\\n\"\n",
    "                            if max_value_excl != '' and u >= max_value_excl:    \n",
    "                                lmsg = msg + \"Warning \" + u + \" found in column \" + k + \" but not allowed per yaml: \" +\\\n",
    "                                str(max_value_excl) + \"\\n\"\n",
    "        st_list.append(df_to_validate)\n",
    "        if len(msg) > 0:\n",
    "            logger.warning(\"Errors found during validation:\")\n",
    "            logger.warning(msg)\n",
    "            if DEBUG:\n",
    "                valid_log_filename = prefix + \"_\" + st + '_validation_errors.log'\n",
    "                errors = open(valid_log_filename, \"w\")\n",
    "                n = errors.write(msg)\n",
    "                errors.close()\n",
    "                logger.warning(\"Validation errors written to \" + valid_log_filename)\n",
    "    valid_df = pd.concat(st_list)\n",
    "    return valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write files out and then do checks, e.g. always debug mode\n",
    "def create_details_file(study_details_df, study_accession,mode='ebi',file_suffix=\"_detail\"):\n",
    "    \"\"\"Returns the details of the EBI/SRA study\n",
    "\n",
    "    If the accession ID is valid, generate a .details.txt, and return the\n",
    "    detail file name of this EBI/SRA study. Else return None\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    study_accession : string\n",
    "        The accession ID of the EBI/SRA study\n",
    "\n",
    "    file_suffix : string\n",
    "        The suffix for the output study detail file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        study details file name\n",
    "    \"\"\"\n",
    "    study_details = study_accession + \"_\" + mode + file_suffix + \".txt\"\n",
    "    study_details_df.to_csv(study_details,sep='\\t',header=True,index=False)\n",
    "    return study_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['collection_device', 'collection_method', 'collection_timestamp', 'description', 'dna_extracted', 'elevation', 'elevation_units', 'empo_1', 'empo_2', 'empo_3', 'env_biome', 'env_feature', 'env_material', 'env_package', 'geo_loc_name', 'host_age', 'host_age_units', 'host_body_habitat', 'host_body_mass_index', 'host_body_product', 'host_body_site', 'host_common_name', 'host_height', 'host_height_units', 'host_scientific_name', 'host_subject_id', 'host_taxid', 'host_weight', 'host_weight_units', 'irb_institute', 'irb_protocol_id', 'latitude', 'latitude_units', 'life_stage', 'longitude', 'longitude_units', 'physical_specimen_location', 'physical_specimen_remaining', 'sample_name', 'sample_type', 'scientific_name', 'sex', 'taxon_id', 'time_point', 'title', 'tube_id'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'human oral metagenome': './human_oral.yml'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG= False\n",
    "test=get_study_details(\"PRJNA575550\")\n",
    "yvd=add_yaml_validators(['./human_oral.yml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_info_files(tmp,'./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-22 16:23:26,657 __main__     INFO     collection_device not found in columns.collection_device has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,657 __main__     INFO     collection_device not found in columns.collection_device has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,657 __main__     INFO     collection_device not found in columns.collection_device has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,660 __main__     INFO     collection_method not found in columns.collection_method has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,660 __main__     INFO     collection_method not found in columns.collection_method has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,660 __main__     INFO     collection_method not found in columns.collection_method has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,666 __main__     INFO     collection_timestamp not found in columns.collection_timestamp has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,666 __main__     INFO     collection_timestamp not found in columns.collection_timestamp has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,666 __main__     INFO     collection_timestamp not found in columns.collection_timestamp has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,671 __main__     INFO     description not found in columns.description has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,671 __main__     INFO     description not found in columns.description has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,671 __main__     INFO     description not found in columns.description has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,675 __main__     INFO     dna_extracted not found in columns.dna_extracted has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,675 __main__     INFO     dna_extracted not found in columns.dna_extracted has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,675 __main__     INFO     dna_extracted not found in columns.dna_extracted has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,679 __main__     INFO     elevation not found in columns.elevation has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,679 __main__     INFO     elevation not found in columns.elevation has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,679 __main__     INFO     elevation not found in columns.elevation has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,686 __main__     INFO     elevation_units not found in columns.Setting elevation_units to meters for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,686 __main__     INFO     elevation_units not found in columns.Setting elevation_units to meters for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,686 __main__     INFO     elevation_units not found in columns.Setting elevation_units to meters for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,689 __main__     INFO     empo_1 not found in columns.Setting empo_1 to Host-associated for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,689 __main__     INFO     empo_1 not found in columns.Setting empo_1 to Host-associated for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,689 __main__     INFO     empo_1 not found in columns.Setting empo_1 to Host-associated for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,693 __main__     INFO     empo_2 not found in columns.Setting empo_2 to Animal for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,693 __main__     INFO     empo_2 not found in columns.Setting empo_2 to Animal for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,693 __main__     INFO     empo_2 not found in columns.Setting empo_2 to Animal for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,697 __main__     INFO     empo_3 not found in columns.Setting empo_3 to animal secretion for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,697 __main__     INFO     empo_3 not found in columns.Setting empo_3 to animal secretion for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,697 __main__     INFO     empo_3 not found in columns.Setting empo_3 to animal secretion for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,702 __main__     INFO     env_biome not found in columns.Setting env_biome to urban biome for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,702 __main__     INFO     env_biome not found in columns.Setting env_biome to urban biome for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,702 __main__     INFO     env_biome not found in columns.Setting env_biome to urban biome for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,705 __main__     INFO     env_feature not found in columns.Setting env_feature to human-associated habitat for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,705 __main__     INFO     env_feature not found in columns.Setting env_feature to human-associated habitat for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,705 __main__     INFO     env_feature not found in columns.Setting env_feature to human-associated habitat for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,709 __main__     INFO     env_material not found in columns.Setting env_material to saliva for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,709 __main__     INFO     env_material not found in columns.Setting env_material to saliva for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,709 __main__     INFO     env_material not found in columns.Setting env_material to saliva for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,714 __main__     INFO     env_package not found in columns.Setting env_package to human-oral for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,714 __main__     INFO     env_package not found in columns.Setting env_package to human-oral for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,714 __main__     INFO     env_package not found in columns.Setting env_package to human-oral for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,723 __main__     INFO     host_age not found in columns.host_age has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,723 __main__     INFO     host_age not found in columns.host_age has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,723 __main__     INFO     host_age not found in columns.host_age has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,727 __main__     INFO     host_age_units not found in columns.Setting host_age_units to years for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,727 __main__     INFO     host_age_units not found in columns.Setting host_age_units to years for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,727 __main__     INFO     host_age_units not found in columns.Setting host_age_units to years for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,732 __main__     INFO     host_body_habitat not found in columns.Setting host_body_habitat to UBERON:oral cavity for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,732 __main__     INFO     host_body_habitat not found in columns.Setting host_body_habitat to UBERON:oral cavity for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,732 __main__     INFO     host_body_habitat not found in columns.Setting host_body_habitat to UBERON:oral cavity for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,736 __main__     INFO     host_body_mass_index not found in columns.host_body_mass_index has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,736 __main__     INFO     host_body_mass_index not found in columns.host_body_mass_index has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,736 __main__     INFO     host_body_mass_index not found in columns.host_body_mass_index has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,739 __main__     INFO     host_body_product not found in columns.Setting host_body_product to UBERON:saliva for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,739 __main__     INFO     host_body_product not found in columns.Setting host_body_product to UBERON:saliva for human oral metagenomesamples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-22 16:23:26,739 __main__     INFO     host_body_product not found in columns.Setting host_body_product to UBERON:saliva for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,745 __main__     INFO     host_body_site not found in columns.Setting host_body_site to UBERON:mouth for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,745 __main__     INFO     host_body_site not found in columns.Setting host_body_site to UBERON:mouth for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,745 __main__     INFO     host_body_site not found in columns.Setting host_body_site to UBERON:mouth for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,748 __main__     INFO     host_common_name not found in columns.Setting host_common_name to human for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,748 __main__     INFO     host_common_name not found in columns.Setting host_common_name to human for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,748 __main__     INFO     host_common_name not found in columns.Setting host_common_name to human for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,752 __main__     INFO     host_height not found in columns.host_height has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,752 __main__     INFO     host_height not found in columns.host_height has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,752 __main__     INFO     host_height not found in columns.host_height has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,756 __main__     INFO     host_height_units not found in columns.Setting host_height_units to cm for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,756 __main__     INFO     host_height_units not found in columns.Setting host_height_units to cm for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,756 __main__     INFO     host_height_units not found in columns.Setting host_height_units to cm for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,761 __main__     INFO     host_scientific_name not found in columns.Setting host_scientific_name to Homo sapiens for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,761 __main__     INFO     host_scientific_name not found in columns.Setting host_scientific_name to Homo sapiens for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,761 __main__     INFO     host_scientific_name not found in columns.Setting host_scientific_name to Homo sapiens for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,764 __main__     INFO     host_subject_id not found in columns.host_subject_id has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,764 __main__     INFO     host_subject_id not found in columns.host_subject_id has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,764 __main__     INFO     host_subject_id not found in columns.host_subject_id has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,769 __main__     INFO     host_taxid not found in columns.Setting host_taxid to 9606 for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,769 __main__     INFO     host_taxid not found in columns.Setting host_taxid to 9606 for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,769 __main__     INFO     host_taxid not found in columns.Setting host_taxid to 9606 for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,772 __main__     INFO     host_weight not found in columns.host_weight has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,772 __main__     INFO     host_weight not found in columns.host_weight has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,772 __main__     INFO     host_weight not found in columns.host_weight has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,776 __main__     INFO     host_weight_units not found in columns.Setting host_weight_units to kg for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,776 __main__     INFO     host_weight_units not found in columns.Setting host_weight_units to kg for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,776 __main__     INFO     host_weight_units not found in columns.Setting host_weight_units to kg for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,779 __main__     INFO     irb_institute not found in columns.irb_institute has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,779 __main__     INFO     irb_institute not found in columns.irb_institute has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,779 __main__     INFO     irb_institute not found in columns.irb_institute has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,783 __main__     INFO     irb_protocol_id not found in columns.irb_protocol_id has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,783 __main__     INFO     irb_protocol_id not found in columns.irb_protocol_id has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,783 __main__     INFO     irb_protocol_id not found in columns.irb_protocol_id has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,787 __main__     INFO     latitude not found in columns.latitude has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,787 __main__     INFO     latitude not found in columns.latitude has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,787 __main__     INFO     latitude not found in columns.latitude has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,790 __main__     INFO     latitude_units not found in columns.Setting latitude_units to Decimal degrees for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,790 __main__     INFO     latitude_units not found in columns.Setting latitude_units to Decimal degrees for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,790 __main__     INFO     latitude_units not found in columns.Setting latitude_units to Decimal degrees for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,794 __main__     INFO     life_stage not found in columns.life_stage has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,794 __main__     INFO     life_stage not found in columns.life_stage has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,794 __main__     INFO     life_stage not found in columns.life_stage has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,798 __main__     INFO     longitude not found in columns.longitude has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,798 __main__     INFO     longitude not found in columns.longitude has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,798 __main__     INFO     longitude not found in columns.longitude has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,805 __main__     INFO     longitude_units not found in columns.Setting longitude_units to Decimal degrees for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,805 __main__     INFO     longitude_units not found in columns.Setting longitude_units to Decimal degrees for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,805 __main__     INFO     longitude_units not found in columns.Setting longitude_units to Decimal degrees for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,809 __main__     INFO     physical_specimen_location not found in columns.physical_specimen_location has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,809 __main__     INFO     physical_specimen_location not found in columns.physical_specimen_location has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,809 __main__     INFO     physical_specimen_location not found in columns.physical_specimen_location has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,813 __main__     INFO     physical_specimen_remaining not found in columns.physical_specimen_remaining has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,813 __main__     INFO     physical_specimen_remaining not found in columns.physical_specimen_remaining has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,813 __main__     INFO     physical_specimen_remaining not found in columns.physical_specimen_remaining has no default, will be encoded as 'not provided'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-22 16:23:26,817 __main__     INFO     sample_type not found in columns.Setting sample_type to saliva for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,817 __main__     INFO     sample_type not found in columns.Setting sample_type to saliva for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,817 __main__     INFO     sample_type not found in columns.Setting sample_type to saliva for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,821 __main__     INFO     sex not found in columns.sex has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,821 __main__     INFO     sex not found in columns.sex has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,821 __main__     INFO     sex not found in columns.sex has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,825 __main__     INFO     taxon_id not found in columns.Setting taxon_id to 447426 for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,825 __main__     INFO     taxon_id not found in columns.Setting taxon_id to 447426 for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,825 __main__     INFO     taxon_id not found in columns.Setting taxon_id to 447426 for human oral metagenomesamples\n",
      "2020-06-22 16:23:26,828 __main__     INFO     time_point not found in columns.time_point has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,828 __main__     INFO     time_point not found in columns.time_point has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,828 __main__     INFO     time_point not found in columns.time_point has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,832 __main__     INFO     title not found in columns.title has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,832 __main__     INFO     title not found in columns.title has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,832 __main__     INFO     title not found in columns.title has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,835 __main__     INFO     tube_id not found in columns.tube_id has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,835 __main__     INFO     tube_id not found in columns.tube_id has no default, will be encoded as 'not provided'\n",
      "2020-06-22 16:23:26,835 __main__     INFO     tube_id not found in columns.tube_id has no default, will be encoded as 'not provided'\n"
     ]
    }
   ],
   "source": [
    "tmp=add_sample_info(test,validator_files=yvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_info_files(final_df,prefix=''):\n",
    "    prep_info_columns = ['run_prefix','experiment_accession','platform','instrument_model','library_strategy',\n",
    "                         'library_source','library_layout','library_selection','fastq_ftp','ena_checklist',\n",
    "                         'ena_spot_count','ena_base_count','ena_first_public','ena_last_update','instrument_platform',\n",
    "                         'submitted_format']\n",
    "    final_df.columns =[scrub_special_chars(col).lower() for col in final_df.columns]\n",
    "    if DEBUG: logger.info(final_df.columns)\n",
    "    #write sample_info\n",
    "    sample_df=final_df[final_df.columns[~final_df.columns.isin(prep_info_columns)]]\n",
    "    sample_df=sample_df.set_index('sample_name').dropna(axis=1,how='all')\n",
    "    sample_df.to_csv(prefix+'_sample_info.tsv',sep='\\t',index=True,index_label='sample_name')\n",
    "    \n",
    "    #clean up pre-validation file assuming correct validation\n",
    "    if not DEBUG:\n",
    "        pre_validation_file = prefix + \"_unvalidated_sample_info.part\"\n",
    "        if path.isfile(pre_validation_file):\n",
    "            remove(pre_validation_file)\n",
    "            \n",
    "    prep_info_columns.append('sample_name') #add to list for writing out prep files\n",
    "    for prep_file in final_df['prep_file']:\n",
    "        prep_df = final_df[final_df['prep_file']==prep_file]\n",
    "        prep_df= prep_df[prep_df.columns[prep_df.columns.isin(prep_info_columns)]].set_index('sample_name')\n",
    "        prep_df=prep_df.dropna(axis=1,how='all')\n",
    "        prep_df.to_csv(prefix+'_prep_info_'+ prep_file + '.tsv',sep='\\t',index=True,index_label='sample_name')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sequencing_data(download_df,output_dir=\"./\",mode='ebi'):\n",
    "    \"\"\"Fetch all the meta file(s) for EBI study\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    study_accession : string\n",
    "        The accession ID of the EBI study\n",
    "\n",
    "    study_details : string\n",
    "        study detail file name\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Downloading the fastqs\")\n",
    "    # Download the fastqs\n",
    "    \n",
    "    for index, row in download_df.iterrows():\n",
    "        if mode == 'ebi':\n",
    "            files = row['fastq_ftp'].split(';')\n",
    "            for f in files:\n",
    "                fq_path = output_dir + \"/\" + f.split('/')[-1]\n",
    "                if DEBUG: logger.info(f)\n",
    "                if type(f) != str:\n",
    "                    logger.warning(\"Skipping sample:\" + row['sample_name']\n",
    "                                   + \", run: \" + row['run_accession'] + \"No fastq ftp found.\")      \n",
    "                elif path.isfile(fq_path):\n",
    "                    logger.warning(\"Skipping \" + fq_path)\n",
    "                    logger.warning(\"File exists\")\n",
    "                else:\n",
    "                    if not path.exists(output_dir):\n",
    "                        makedirs(output_dirh)\n",
    "                    urlretrieve(\"ftp://\" +f, fq_path)\n",
    "        elif mode =='sra':\n",
    "            subprocess.run(['fastq-dump', '-I', '--split-files', '--gzip', row['run_accession']])\n",
    "        else:\n",
    "            raise Exception(mode + \" is not a valid repository\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-o OUTPUT] [-mode {ebi,sra}]\n",
      "                             [-prefix PREFIX]\n",
      "                             [-strat [{POOLCLONE,CLONE,CLONEEND,WGS,WGA,WCS,WXS,AMPLICON,ChIP-Seq,RNA-Seq,MRE-Seq,MeDIP-Seq,MBD-Seq,MNase-Seq,DNase-Hypersensitivity,Bisulfite-Seq,EST,FL-cDNA,miRNA-Seq,ncRNA-Seq,FINISHING,TS,Tn-Seq,VALIDATION,FAIRE-seq,SELEX,RIP-Seq,ChIA-PET,RAD-Seq} [{POOLCLONE,CLONE,CLONEEND,WGS,WGA,WCS,WXS,AMPLICON,ChIP-Seq,RNA-Seq,MRE-Seq,MeDIP-Seq,MBD-Seq,MNase-Seq,DNase-Hypersensitivity,Bisulfite-Seq,EST,FL-cDNA,miRNA-Seq,ncRNA-Seq,FINISHING,TS,Tn-Seq,VALIDATION,FAIRE-seq,SELEX,RIP-Seq,ChIA-PET,RAD-Seq} ...]]]\n",
      "                             [-plat [{LS454,Illumina,Ion Torrent,PacBio_SMRT,OXFORD_NANOPORE} [{LS454,Illumina,Ion Torrent,PacBio_SMRT,OXFORD_NANOPORE} ...]]]\n",
      "                             [-yaml [VALIDATORS [VALIDATORS ...]]]\n",
      "                             [-yaml-dir YAML_DIR] [-no-seqs] [-sep SEP] [-v]\n",
      "                             [P [P ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adswafford/miniconda3/envs/ebi_sra_importer/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # parse the flags and initialize output file names\n",
    "    # TODO:Reword help info\n",
    "    parser = ArgumentParser(description='Please note that the following ' +\n",
    "                            'packages have to be installed for running this ' +\n",
    "                            'script: 1)lxml 2)pandas 3)glob 4)csv 5)sys ' +\n",
    "                            '6)urllib 7)argparse 8)requests 9)xmltodict ' +\n",
    "                            '10)subprocess 11)bioconda 12)sra-tools 13)os ' +\n",
    "                            '14)entrez-direct 15)pyyaml')\n",
    "    parser.add_argument(\"-project\",\"--project\", nargs='*',help=\"EBI/ENA project or study accession(s) \" +\n",
    "                        \"to retrieve\")\n",
    "    parser.add_argument(\"-o\",\"--output\", default='./',help='directory for output files. Default is working directory.')\n",
    "    parser.add_argument(\"-mode\", \"--mode\", default='ebi', help=\"sra accession \" +\n",
    "                        \"repository to be queried.\", choices=['ebi','sra'])\n",
    "    parser.add_argument(\"-prefix\", \"--prefix\", nargs='*', help=\"prefix(es) to prepend to output info files\")\n",
    "    parser.add_argument(\"-strat\",\"--strategies\",nargs='*',choices=['POOLCLONE','CLONE','CLONEEND','WGS','WGA',\n",
    "                                                       'WCS','WXS','AMPLICON','ChIP-Seq','RNA-Seq',\n",
    "                                                       'MRE-Seq','MeDIP-Seq','MBD-Seq','MNase-Seq',\n",
    "                                                       'DNase-Hypersensitivity','Bisulfite-Seq','EST',\n",
    "                                                       'FL-cDNA','miRNA-Seq','ncRNA-Seq','FINISHING',\n",
    "                                                       'TS','Tn-Seq','VALIDATION','FAIRE-seq','SELEX',\n",
    "                                                       'RIP-Seq','ChIA-PET','RAD-Seq'],\n",
    "                        help=\"list of one or more libary strategies to restrict selection.\")\n",
    "    parser.add_argument(\"-plat\", \"--platforms\", nargs='*', choices=['LS454','Illumina','Ion Torrent','PacBio_SMRT','OXFORD_NANOPORE'],\n",
    "                        help=\"List of one or more platforms to restrict selection.\")\n",
    "    parser.add_argument(\"-name\",\"--scientific_names\",nargs='*',help=\"List of scientific_names to restrict for selection.\")\n",
    "    parser.add_argument(\"-yaml\",\"--validators\",nargs='*', help=\"one or more yaml files in QIIMP format for validation.\")\n",
    "    parser.add_argument(\"-yaml-dir\",\"--yaml_dir\", default ='./', help=\"one or more yaml files in QIIMP format for validation.\")\n",
    "    parser.add_argument(\"-no-seqs\", \"--no_seqs\", default=False,action='store_true', help=\"Omit download of fastq files.\")\n",
    "    parser.add_argument(\"-sep\",\"--sep\",default=';',help=\"separator for parsing description, default is ';' \")\n",
    "    parser.add_argument(\"-v\", \"--verbose\", default=False, action='store_true', help=\"Output additional messages.\")\n",
    "    parser.add_argument(\"-log\", \"--log\", default='./output.log',help=\"filename for logger.\")\n",
    "    parser.add_argument(\"-f\",\"--force\",default=False, action='store_true', help=\"Advanced: force use of specified yaml for validation.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.project is None:\n",
    "        logger.warning(\"\"\"\n",
    "                python EBI_SRA_Downloader.py -project [accession] [accession ... N]\n",
    "                    Generate the study info, study detail, prep, and  sample\n",
    "                    files for the entered EBI accession, and download the\n",
    "                    FASTQ files.\n",
    "                Optional flags:\n",
    "                    -output [directory where files will be saved]\n",
    "                    -mode [specifies which repository to use]                    \n",
    "                    -prefix [list of prefixes for sample and prep info files]\n",
    "                    --strategy [list of one or more library strategies to select]\n",
    "                    --platforms [list of one or more sequencing platforms to select]\n",
    "                    --validators [list of one or more yaml files to use in validating]\n",
    "                    --no_seqs [skip downloading files]\n",
    "                    --verbose          \n",
    "                    --sep [provide a delimiter for parsing the description field]\n",
    "               \"\"\")\n",
    "        sys.exit(2)\n",
    "    else:\n",
    "        #settings\n",
    "        mode=args.mode\n",
    "        sep= args.sep\n",
    "        DEBUG = args.verbose\n",
    "        omit_seqs = args.no_seqs\n",
    "        force = args.force\n",
    "        \n",
    "        #set up logging\n",
    "        handler = logging.StreamHandler()\n",
    "        fmt_str = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    "        handler.setFormatter(logging.Formatter(fmt_str))\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.addHandler(handler)\n",
    "                \n",
    "        fh=logging.FileHandler(args.log)\n",
    "        logger.addHandler(fh)\n",
    "        if DEBUG: logger.setLevel(logging.INFO)\n",
    "   \n",
    "        # Output directory\n",
    "        output = args.output\n",
    "        if list(output)[-1] != '/':\n",
    "            output = output + '/'\n",
    "        \n",
    "        #set up validators\n",
    "        yaml_validator_dict = {}\n",
    "        yaml_list =  []\n",
    "        \n",
    "        if args.validators is not None:\n",
    "            for y in args.validators:\n",
    "                yaml_list.append(y)\n",
    "        else:\n",
    "            for file in os.listdir(args.yaml_dir):\n",
    "                if file.endswith(\".yml\") or file.endswith(\".yaml\"):\n",
    "                    yaml_list.append(os.path.join(args.yaml_dir, file))\n",
    "        \n",
    "        #since we're provding a way to override parsing, check assumption that a single validator is being passed\n",
    "        if force:\n",
    "            if len(yaml_list) > 1:\n",
    "                raise Exception(\"Error: more than one yaml supplied with 'force' mode. Please supply only one yaml file.\" +\\\n",
    "                                \"Note .yml and .yaml files in working directory (or specified by --yaml_dir) are loaded if \" +\\\n",
    "                                \" no -yaml flag is supplied. Loaded: \" + str(yaml_list))\n",
    "            elif len(yaml_list) == 0:\n",
    "                raise Exception(\"Error: no yaml supplied with 'force' mode. Please supply only one yaml file.\")\n",
    "            else:\n",
    "                yaml_validator_dict = yaml_list\n",
    "        else:\n",
    "            yaml_validator_dict = add_yaml_validators(yaml_list)\n",
    "                       \n",
    "        platforms=[]\n",
    "        if args.platforms is not None:\n",
    "            for p in args.platforms:\n",
    "                platforms.append(p.lower())\n",
    "        \n",
    "        strategies =[]\n",
    "        if args.strategies is not None:\n",
    "            for s in args.strategies:\n",
    "                strategies.append(s.lower())\n",
    "        \n",
    "        names=[]\n",
    "        if args.scientific_names is not None:\n",
    "            for n in args.scientific_names:\n",
    "                names.append(n.lower())\n",
    "        \n",
    "        # Retreive study information\n",
    "        p_count=0\n",
    "        for p in args.project:\n",
    "            if args.prefix is not None:\n",
    "                if len(args.prefix) == 1:\n",
    "                    file_prefix = output + args.prefix[0] + '_' + p\n",
    "                elif len(args.prefix) == len(args.project):\n",
    "                    file_prefix = output + args.prefix[p_count] + '_' + p\n",
    "                else:\n",
    "                    raise Exception(\"Number of prefixes does not match number of projects. Set a single prefix or matched prefixes.\")\n",
    "            else:\n",
    "                file_prefix = p\n",
    "            p_count +=1    \n",
    "            study = get_study_details(p,mode,file_prefix)\n",
    "            \n",
    "            #tidy metadata\n",
    "            md=get_sample_info(study,mode,platforms,strategies,yaml_validator_dict,file_prefix,names)\n",
    "            \n",
    "            #write out files            \n",
    "            write_info_files(md,file_prefix)\n",
    "            \n",
    "            if not omit_seqs:\n",
    "                fetch_sequencing_data(md,output,mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
